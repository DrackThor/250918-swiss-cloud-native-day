policies:
  - uid: cis-kubernetes-master-level-1
    owner_mrn: //policy.api.mondoo.app
    name: CIS Kubernetes Benchmark - Level 1 - Master Node
    version: 1.9.0
    tags:
      mondoo.com/category: compliance
      mondoo.com/platform: kubernetes,linux
    props:
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/adminConfFiles
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.13/properties/adminConfFiles
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.14/properties/adminConfFiles
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedCiphers
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.2.29/properties/allowedCiphers
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToCreatePodsViaClusterrolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.4/properties/allowedSubjectsToCreatePodsViaClusterrolebinding
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.9/properties/allowedSubjectsToCreatePodsViaClusterrolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToCreatePodsViaRolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.4/properties/allowedSubjectsToCreatePodsViaRolebinding
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.9/properties/allowedSubjectsToCreatePodsViaRolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToEscalateViaClusterrolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.8/properties/allowedSubjectsToEscalateViaClusterrolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToEscalateViaRolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.8/properties/allowedSubjectsToEscalateViaRolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToGetSecretsViaClusterrolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.2/properties/allowedSubjectsToGetSecretsViaClusterrolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToGetSecretsViaRolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.2/properties/allowedSubjectsToGetSecretsViaRolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveCSRApprovalRightsViaClusterrolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.11/properties/allowedSubjectsToHaveCSRApprovalRightsViaClusterrolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveCSRApprovalRightsViaRolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.11/properties/allowedSubjectsToHaveCSRApprovalRightsViaRolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveClusterAdminRole
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.1/properties/allowedSubjectsToHaveClusterAdminRole
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveNodeProxyAccessViaClusterrolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.10/properties/allowedSubjectsToHaveNodeProxyAccessViaClusterrolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveNodeProxyAccessViaRolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.10/properties/allowedSubjectsToHaveNodeProxyAccessViaRolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveSATokenRightsViaClusterrolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.13/properties/allowedSubjectsToHaveSATokenRightsViaClusterrolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveSATokenRightsViaRolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.13/properties/allowedSubjectsToHaveSATokenRightsViaRolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveWebHookRightsViaClusterrolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.12/properties/allowedSubjectsToHaveWebHookRightsViaClusterrolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveWebHookRightsViaRolebinding
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.12/properties/allowedSubjectsToHaveWebHookRightsViaRolebinding
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/apiServerPodSpecFile
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.1/properties/apiServerPodSpecFile
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/checkRolebindingsBoolean
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.1/properties/checkRolebindingsBoolean
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/containerNetworkInterfaceFile
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.9/properties/containerNetworkInterfaceFile
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.10/properties/containerNetworkInterfaceFile
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/controllerManagerConfFile
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.17/properties/controllerManagerConfFile
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/etcdDataDir
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.11/properties/etcdDataDir
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.12/properties/etcdDataDir
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.5/properties/excludedNamespaces
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.6/properties/excludedNamespaces
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces1
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.2/properties/excludedNamespaces1
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces10
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.11/properties/excludedNamespaces10
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces11
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.12/properties/excludedNamespaces11
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces12
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.13/properties/excludedNamespaces12
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces2
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.3/properties/excludedNamespaces2
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces3
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.4/properties/excludedNamespaces3
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces4
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.5/properties/excludedNamespaces4
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces5
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.6/properties/excludedNamespaces5
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces7
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.9/properties/excludedNamespaces7
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces9
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.8/properties/excludedNamespaces9
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespacesPolicyControl
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.1/properties/excludedNamespacesPolicyControl
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/ignoredNamespaces
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.1/properties/ignoredNamespaces
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.2/properties/ignoredNamespaces
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.4/properties/ignoredNamespaces
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.8/properties/ignoredNamespaces
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.9/properties/ignoredNamespaces
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.10/properties/ignoredNamespaces
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.11/properties/ignoredNamespaces
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.12/properties/ignoredNamespaces
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.13/properties/ignoredNamespaces
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubeControllerManager
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.3/properties/kubeControllerManager
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubeEtcdPodSpecFile
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.7/properties/kubeEtcdPodSpecFile
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubePkiDictPath
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.19/properties/kubePkiDictPath
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubeSchedulerFile
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.5/properties/kubeSchedulerFile
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubeletServiceFile
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--4.1.1/properties/kubeletServiceFile
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/pkiCertificateFilePaths
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.20/properties/pkiCertificateFilePaths
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.21/properties/pkiCertificateFilePaths
      - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/scheduleConfFile
        for:
          - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.15/properties/scheduleConfFile
    authors:
      - name: Mondoo, Inc.
        email: hello@mondoo.com
    docs:
      desc: This policy is based off of the CIS Kubernetes Benchmark Level 1 Profile. Each of the automated controls from the benchmark have been translated to the Mondoo Query Language (MQL), while manual controls are not covered. The policy provides prescriptive guidance for configuring security options Kubernetes v1.27 - v1.29 master nodes. with an emphasis on foundational, testable, and architecture agnostic settings.
    groups:
      - uid: cis-kubernetes-master-level-1--1-1
        title: Control Plane Node Configuration Files
        filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty && asset.kind != "container-image"
        checks:
          - uid: cis-kubernetes--1.1.1
          - uid: cis-kubernetes--1.1.2
          - uid: cis-kubernetes--1.1.3
          - uid: cis-kubernetes--1.1.4
          - uid: cis-kubernetes--1.1.5
          - uid: cis-kubernetes--1.1.6
          - uid: cis-kubernetes--1.1.7
          - uid: cis-kubernetes--1.1.8
          - uid: cis-kubernetes--1.1.9
          - uid: cis-kubernetes--1.1.10
          - uid: cis-kubernetes--1.1.11
          - uid: cis-kubernetes--1.1.12
          - uid: cis-kubernetes--1.1.13
          - uid: cis-kubernetes--1.1.14
          - uid: cis-kubernetes--1.1.15
          - uid: cis-kubernetes--1.1.16
          - uid: cis-kubernetes--1.1.17
          - uid: cis-kubernetes--1.1.18
          - uid: cis-kubernetes--1.1.19
          - uid: cis-kubernetes--1.1.20
          - uid: cis-kubernetes--1.1.21
      - uid: cis-kubernetes-master-level-1--1-2
        title: API Server
        filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty && asset.kind != "container-image"
        checks:
          - uid: cis-kubernetes--1.2.1
          - uid: cis-kubernetes--1.2.2
          - uid: cis-kubernetes--1.2.3
          - uid: cis-kubernetes--1.2.4
          - uid: cis-kubernetes--1.2.5
          - uid: cis-kubernetes--1.2.6
          - uid: cis-kubernetes--1.2.7
          - uid: cis-kubernetes--1.2.8
          - uid: cis-kubernetes--1.2.9
          - uid: cis-kubernetes--1.2.10
          - uid: cis-kubernetes--1.2.11
          - uid: cis-kubernetes--1.2.15
          - uid: cis-kubernetes--1.2.16
          - uid: cis-kubernetes--1.2.17
          - uid: cis-kubernetes--1.2.18
          - uid: cis-kubernetes--1.2.19
          - uid: cis-kubernetes--1.2.20
          - uid: cis-kubernetes--1.2.21
          - uid: cis-kubernetes--1.2.22
          - uid: cis-kubernetes--1.2.23
          - uid: cis-kubernetes--1.2.24
          - uid: cis-kubernetes--1.2.25
          - uid: cis-kubernetes--1.2.26
          - uid: cis-kubernetes--1.2.27
          - uid: cis-kubernetes--1.2.28
          - uid: cis-kubernetes--1.2.29
      - uid: cis-kubernetes-master-level-1--1-3
        title: Controller Manager
        filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty && asset.kind != "container-image"
        checks:
          - uid: cis-kubernetes--1.3.1
          - uid: cis-kubernetes--1.3.2
          - uid: cis-kubernetes--1.3.3
          - uid: cis-kubernetes--1.3.4
          - uid: cis-kubernetes--1.3.5
          - uid: cis-kubernetes--1.3.6
          - uid: cis-kubernetes--1.3.7
      - uid: cis-kubernetes-master-level-1--1-4
        title: Scheduler
        filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty && asset.kind != "container-image"
        checks:
          - uid: cis-kubernetes--1.4.1
          - uid: cis-kubernetes--1.4.2
      - uid: cis-kubernetes-master-level-1--2
        title: etcd
        filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty && asset.kind != "container-image"
        checks:
          - uid: cis-kubernetes--2.1
          - uid: cis-kubernetes--2.2
          - uid: cis-kubernetes--2.3
          - uid: cis-kubernetes--2.4
          - uid: cis-kubernetes--2.5
          - uid: cis-kubernetes--2.6
      - uid: cis-kubernetes-master-level-1--3-2
        title: Logging
        filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty && asset.kind != "container-image"
        checks:
          - uid: cis-kubernetes--3.2.1
      - uid: cis-kubernetes-master-level-1--4-1
        title: Worker Node Configuration Files
        filters: asset.family.contains('linux') && processes.where( executable == /kubelet/ ).list != empty && asset.kind != "container-image"
        checks:
          - uid: cis-kubernetes--4.1.1
      - uid: cis-kubernetes-master-level-1--5-1
        title: RBAC and Service Accounts
        filters: asset.platform == "k8s-cluster"
        checks:
          - uid: cis-kubernetes--5.1.1
          - uid: cis-kubernetes--5.1.2
          - uid: cis-kubernetes--5.1.4
          - uid: cis-kubernetes--5.1.5
          - uid: cis-kubernetes--5.1.6
          - uid: cis-kubernetes--5.1.8
          - uid: cis-kubernetes--5.1.9
          - uid: cis-kubernetes--5.1.10
          - uid: cis-kubernetes--5.1.11
          - uid: cis-kubernetes--5.1.12
          - uid: cis-kubernetes--5.1.13
      - uid: cis-kubernetes-master-level-1--5-2
        title: Pod Security Standards
        checks:
          - uid: cis-kubernetes--5.2.1
          - uid: cis-kubernetes--5.2.2
          - uid: cis-kubernetes--5.2.3
          - uid: cis-kubernetes--5.2.4
          - uid: cis-kubernetes--5.2.5
          - uid: cis-kubernetes--5.2.6
          - uid: cis-kubernetes--5.2.8
          - uid: cis-kubernetes--5.2.9
          - uid: cis-kubernetes--5.2.11
          - uid: cis-kubernetes--5.2.12
          - uid: cis-kubernetes--5.2.13
    scoring_system: highest impact
props:
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/adminConfFiles
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.13/properties/adminConfFiles
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.14/properties/adminConfFiles
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedCiphers
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.2.29/properties/allowedCiphers
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToCreatePodsViaClusterrolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.4/properties/allowedSubjectsToCreatePodsViaClusterrolebinding
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.9/properties/allowedSubjectsToCreatePodsViaClusterrolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToCreatePodsViaRolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.4/properties/allowedSubjectsToCreatePodsViaRolebinding
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.9/properties/allowedSubjectsToCreatePodsViaRolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToEscalateViaClusterrolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.8/properties/allowedSubjectsToEscalateViaClusterrolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToEscalateViaRolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.8/properties/allowedSubjectsToEscalateViaRolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToGetSecretsViaClusterrolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.2/properties/allowedSubjectsToGetSecretsViaClusterrolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToGetSecretsViaRolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.2/properties/allowedSubjectsToGetSecretsViaRolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveCSRApprovalRightsViaClusterrolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.11/properties/allowedSubjectsToHaveCSRApprovalRightsViaClusterrolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveCSRApprovalRightsViaRolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.11/properties/allowedSubjectsToHaveCSRApprovalRightsViaRolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveClusterAdminRole
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.1/properties/allowedSubjectsToHaveClusterAdminRole
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveNodeProxyAccessViaClusterrolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.10/properties/allowedSubjectsToHaveNodeProxyAccessViaClusterrolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveNodeProxyAccessViaRolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.10/properties/allowedSubjectsToHaveNodeProxyAccessViaRolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveSATokenRightsViaClusterrolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.13/properties/allowedSubjectsToHaveSATokenRightsViaClusterrolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveSATokenRightsViaRolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.13/properties/allowedSubjectsToHaveSATokenRightsViaRolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveWebHookRightsViaClusterrolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.12/properties/allowedSubjectsToHaveWebHookRightsViaClusterrolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/allowedSubjectsToHaveWebHookRightsViaRolebinding
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.12/properties/allowedSubjectsToHaveWebHookRightsViaRolebinding
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/apiServerPodSpecFile
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.1/properties/apiServerPodSpecFile
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/checkRolebindingsBoolean
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.1/properties/checkRolebindingsBoolean
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/containerNetworkInterfaceFile
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.9/properties/containerNetworkInterfaceFile
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.10/properties/containerNetworkInterfaceFile
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/controllerManagerConfFile
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.17/properties/controllerManagerConfFile
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/etcdDataDir
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.11/properties/etcdDataDir
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.12/properties/etcdDataDir
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.5/properties/excludedNamespaces
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.6/properties/excludedNamespaces
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces1
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.2/properties/excludedNamespaces1
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces10
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.11/properties/excludedNamespaces10
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces11
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.12/properties/excludedNamespaces11
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces12
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.13/properties/excludedNamespaces12
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces2
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.3/properties/excludedNamespaces2
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces3
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.4/properties/excludedNamespaces3
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces4
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.5/properties/excludedNamespaces4
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces5
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.6/properties/excludedNamespaces5
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces7
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.9/properties/excludedNamespaces7
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespaces9
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.8/properties/excludedNamespaces9
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/excludedNamespacesPolicyControl
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.2.1/properties/excludedNamespacesPolicyControl
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/ignoredNamespaces
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.1/properties/ignoredNamespaces
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.2/properties/ignoredNamespaces
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.4/properties/ignoredNamespaces
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.8/properties/ignoredNamespaces
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.9/properties/ignoredNamespaces
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.10/properties/ignoredNamespaces
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.11/properties/ignoredNamespaces
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.12/properties/ignoredNamespaces
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--5.1.13/properties/ignoredNamespaces
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubeControllerManager
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.3/properties/kubeControllerManager
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubeEtcdPodSpecFile
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.7/properties/kubeEtcdPodSpecFile
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubePkiDictPath
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.19/properties/kubePkiDictPath
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubeSchedulerFile
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.5/properties/kubeSchedulerFile
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/kubeletServiceFile
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--4.1.1/properties/kubeletServiceFile
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/pkiCertificateFilePaths
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.20/properties/pkiCertificateFilePaths
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.21/properties/pkiCertificateFilePaths
  - mrn: //policy.api.mondoo.app/policies/cis-kubernetes-master-level-1/properties/scheduleConfFile
    for:
      - mrn: //policy.api.mondoo.app/queries/cis-kubernetes--1.1.15/properties/scheduleConfFile
queries:
  - uid: cis-kubernetes--1.1.1
    title: Ensure that the API server pod specification file permissions are set to 600 or more restrictive
    impact: 60
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.1
    props:
      - uid: cis-kubernetes--1.1.1
        title: Define possible locations of the API server pod specification file
        mql: |
          return [
          "/etc/kubernetes/manifests/kube-apiserver.yaml",
          ]
    mql: |
      props.apiServerPodSpecFile.any(file(_).exists)
      props.apiServerPodSpecFile.where(file(_).exists) {
        file (_) {
          path
          permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that the API server pod specification file has permissions of `600` or more restrictive.

        **Rationale:**

        The API server pod specification file controls various parameters that set the behavior of the API server. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %a /etc/kubernetes/manifests/kube-apiserver.yaml
        ```

        Verify that the permissions are `600` or more restrictive.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml
            ```
  - uid: cis-kubernetes--1.1.10
    title: Ensure that the Container Network Interface file ownership is set to root:root
    impact: 80
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.10
    props:
      - uid: cis-kubernetes--1.1.10
        title: Defines the possible locations of the Container Network Interface file
        mql: |
          return
          [
            "/etc/cni/net.d",
            kubelet.process.flags["cni-conf-dir"],
          ]
    mql: |
      props.containerNetworkInterfaceFile.any(file(_).exists)
      props.containerNetworkInterfaceFile.where(file(_).exists) {
        files.find(from: _ , type: 'file',).where(basename == /\.conf$|\.conflist$/) {
          path
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |-
        Ensure that the Container Network Interface files have ownership set to `root:root`.

        **Rationale:**

        Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be owned by `root:root`.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %U:%G <path/to/cni/files>
        ```

        Verify that the ownership is set to `root:root`.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chown root:root <path/to/cni/files>
            ```
  - uid: cis-kubernetes--1.1.11
    title: Ensure that the etcd data directory permissions are set to 700 or more restrictive
    impact: 70
    filters: processes.where(executable == /etcd/).list != empty
    tags:
      cisecurity.org/recommendation: 1.1.11
    props:
      - uid: cis-kubernetes--1.1.11
        title: Define directory location of ['data-dir'] flag, from etcd
        mql: |
          return [ "/var/lib/etcd",
            processes.where(executable == /etcd/).list.first.flags["data-dir"],
          ]
    mql: |
      props.etcdDataDir.any(file(_).exists)
      props.etcdDataDir.where(file(_).exists) {
        file(_) {
          path
          permissions {
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that the etcd data directory has permissions of `700` or more restrictive.

        **Rationale:**

        etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should not be readable or writable by any group members or the world.

        **Impact:**

        None
      audit: |-
        On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:

        ```
        ps -ef | grep etcd
        ```

        Run the below command (based on the etcd data directory found above). For example,

        ```
        stat -c %a /var/lib/etcd
        ```

        Verify that the permissions are `700` or more restrictive.
      remediation:
        - desc: |-
            On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:

            ```
            ps -ef | grep etcd
            ```
            Run the below command (based on the etcd data directory found above). For example,
            ```
            chmod 700 /var/lib/etcd
            ```
  - uid: cis-kubernetes--1.1.12
    title: Ensure that the etcd data directory ownership is set to etcd:etcd
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.12
    props:
      - uid: cis-kubernetes--1.1.12
        title: Defines the location of the etcd data directory
        mql: |
          return
          [
            processes.where(executable == /etcd/).list.first.flags["data-dir"],
            "/var/lib/etcd"
          ]
    mql: |
      props.etcdDataDir.any(file(_).exists)
      props.etcdDataDir.where(file(_).exists) {
        file (_) {
          path
          user.name == "etcd"
          group.name == "etcd"
        }
      }
    docs:
      desc: |-
        Ensure that the etcd data directory ownership is set to `etcd:etcd`.

        **Rationale:**

        etcd is a highly-available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. This data directory should be protected from any unauthorized reads or writes. It should be owned by `etcd:etcd`.

        **Impact:**

        None
      audit: |-
        On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:

        ```
        ps -ef | grep etcd
        ```

        Run the below command (based on the etcd data directory found above). For example,

        ```
        stat -c %U:%G /var/lib/etcd
        ```

        Verify that the ownership is set to `etcd:etcd`.
      remediation:
        - desc: |-
            On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:

            ```
            ps -ef | grep etcd
            ```
            Run the below command (based on the etcd data directory found above). For example,
            ```
            chown etcd:etcd /var/lib/etcd
            ```
  - uid: cis-kubernetes--1.1.13
    title: Ensure that the default administrative credential file permissions are set to 600
    impact: 70
    filters: processes.where(executable == /kube-apiserver/).list != empty
    tags:
      cisecurity.org/recommendation: 1.1.13
    props:
      - uid: cis-kubernetes--1.1.13
        title: Defines the location of the etcd data directory
        mql: |
          return
          [
            "/etc/kubernetes/admin.conf",
            "/etc/kubernetes/super-admin.conf"
          ]
    mql: |
      props.adminConfFiles.any(file(_).exists)
      props.adminConfFiles.where(file(_).exists) {
        file(_).permissions {
          user_readable == true
          user_executable == false
          group_readable == false
          group_writeable == false
          group_executable == false
          other_readable == false
          other_writeable == false
          other_executable == false
        }
      }
    docs:
      desc: |-
        Ensure that the `admin.conf` file (and `super-admin.conf` file, where it exists) have permissions of `600`.

        **Rationale:**

        As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should restrict their file permissions to maintain the integrity and confidentiality of the file(s). The file(s) should be readable and writable by only the administrators on the system.

        **Impact:**

        None.
      audit: |-
        Run the following command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %a /etc/kubernetes/admin.conf
        ```

        On Kubernetes version 1.29 and higher run the following command as well :-

        ```
        stat -c %a /etc/kubernetes/super-admin.conf
        ```

        Verify that the permissions are `600` or more restrictive.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod 600 /etc/kubernetes/admin.conf
            ```

            On Kubernetes 1.29+ the `super-admin.conf` file should also be modified, if present. For example,

            ```
            chmod 600 /etc/kubernetes/super-admin.conf
            ```
  - uid: cis-kubernetes--1.1.14
    title: Ensure that the default administrative credential file ownership is set to root:root
    impact: 70
    filters: processes.where(executable == /kubelet/).list != empty
    tags:
      cisecurity.org/recommendation: 1.1.14
    props:
      - uid: cis-kubernetes--1.1.14
        title: Defines the location of the etcd data directory
        mql: |
          return
          [
            "/etc/kubernetes/admin.conf",
            "/etc/kubernetes/super-admin.conf"
          ]
    mql: |
      props.adminConfFiles.any(file(_).exists)
      props.adminConfFiles.where(file(_).exists) {
        file(_) {
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |-
        Ensure that the `admin.conf` (and `super-admin.conf` file, where it exists) file ownership is set to `root:root`.

        **Rationale:**

        As part of initial cluster setup, default kubeconfig files are created to be used by the administrator of the cluster. These files contain private keys and certificates which allow for privileged access to the cluster. You should set their file ownership to maintain the integrity and confidentiality of the file. The file(s) should be owned by root:root.

        **Impact:**

        None.
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %U:%G /etc/kubernetes/admin.conf
        ```

        On Kubernetes version 1.29 and higher run the following command as well :-

        ```
        stat -c %a /etc/kubernetes/super-admin.conf
        ```

        Verify that the ownership is set to `root:root`.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chown root:root /etc/kubernetes/admin.conf
            ```

            On Kubernetes 1.29+ the super-admin.conf file should also be modified, if present. For example,

            ```
            chown root:root /etc/kubernetes/super-admin.conf
            ```
  - uid: cis-kubernetes--1.1.15
    title: Ensure that the scheduler.conf file permissions are set to 600 or more restrictive
    impact: 60
    filters: processes.where(executable == /kube-scheduler/).list != empty
    tags:
      cisecurity.org/recommendation: 1.1.15
    props:
      - uid: cis-kubernetes--1.1.15
        title: Define possible locations of the scheduler.conf file
        mql: |
          return [
          "/etc/kubernetes/scheduler.conf",
          ]
    mql: |
      props.scheduleConfFile.any(file(_).exists)
      props.scheduleConfFile.where(file(_).exists) {
        file (_) {
          path
          permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that the `scheduler.conf` file has permissions of `600` or more restrictive.

        **Rationale:**

        The `scheduler.conf` file is the kubeconfig file for the Scheduler. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.

        **Impact:**

        None
      audit: |-
        Run the following command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %a /etc/kubernetes/scheduler.conf
        ```

        Verify that the permissions are `600` or more restrictive.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod 600 /etc/kubernetes/scheduler.conf
            ```
  - uid: cis-kubernetes--1.1.16
    title: Ensure that the scheduler.conf file ownership is set to root:root
    impact: 60
    filters: processes.where(executable == /kube-scheduler/).list != empty
    tags:
      cisecurity.org/recommendation: 1.1.16
    mql: |
      file("/etc/kubernetes/scheduler.conf") {
        user.name == "root"
        group.name == "root"
      }
    docs:
      desc: |-
        Ensure that the `scheduler.conf` file ownership is set to `root:root`.

        **Rationale:**

        The `scheduler.conf` file is the kubeconfig file for the Scheduler. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %U:%G /etc/kubernetes/scheduler.conf
        ```

        Verify that the ownership is set to `root:root`.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chown root:root /etc/kubernetes/scheduler.conf
            ```
  - uid: cis-kubernetes--1.1.17
    title: Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive
    impact: 60
    filters: processes.where(executable == /kube-controller-manager/).list != empty
    tags:
      cisecurity.org/recommendation: 1.1.17
    props:
      - uid: cis-kubernetes--1.1.17
        title: Define the location of the controller-manager.conf file
        mql: |
          return [
           "/etc/kubernetes/controller-manager.conf",
          ]
    mql: |
      props.controllerManagerConfFile.any(file(_).exists)
      props.controllerManagerConfFile.where(file(_).exists) {
        file (_) {
          path
          permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that the `controller-manager.conf` file has permissions of 600 or more restrictive.

        **Rationale:**

        The `controller-manager.conf` file is the kubeconfig file for the Controller Manager. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.

        **Impact:**

        None
      audit: |-
        Run the following command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %a /etc/kubernetes/controller-manager.conf
        ```

        Verify that the permissions are `600` or more restrictive.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod 600 /etc/kubernetes/controller-manager.conf
            ```
  - uid: cis-kubernetes--1.1.18
    title: Ensure that the controller-manager.conf file ownership is set to root:root
    impact: 60
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.18
    mql: |
      file("/etc/kubernetes/controller-manager.conf") {
        user.name == "root"
        group.name == "root"
      }
    docs:
      desc: |-
        Ensure that the `controller-manager.conf` file ownership is set to `root:root`.

        **Rationale:**

        The `controller-manager.conf` file is the kubeconfig file for the Controller Manager. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %U:%G /etc/kubernetes/controller-manager.conf
        ```

        Verify that the ownership is set to `root:root`.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chown root:root /etc/kubernetes/controller-manager.conf
            ```
  - uid: cis-kubernetes--1.1.19
    title: Ensure that the Kubernetes PKI directory and file ownership is set to root:root
    impact: 75
    filters: processes.where(executable == /kube-apiserver/).list != empty
    tags:
      cisecurity.org/recommendation: 1.1.19
    props:
      - uid: cis-kubernetes--1.1.19
        title: Define the possible Kubernetes PKI directory path
        mql: |
          return [
            "/etc/kubernetes/ssl/",
            "/var/lib/minikube/certs/",
            "/etc/kubernetes/pki/"
          ]
    mql: |
      props.kubePkiDictPath.any(file(_).exists)
      props.kubePkiDictPath.where(file(_).exists) {
        file(_) {
          path
          user.name == "root"
          group.name == "root"
        }
      }
    docs:
      desc: |-
        Ensure that the Kubernetes PKI directory and file ownership is set to `root:root`.

        **Rationale:**

        Kubernetes makes use of a number of certificates as part of its operation. You should set the ownership of the directory containing the PKI information and all files in that directory to maintain their integrity. The directory and files should be owned by `root:root`.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        ls -laR /etc/kubernetes/pki/
        ```

        Verify that the ownership of all files and directories in this hierarchy is set to `root:root`.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chown -R root:root /etc/kubernetes/pki/
            ```
  - uid: cis-kubernetes--1.1.2
    title: Ensure that the API server pod specification file ownership is set to root:root
    impact: 60
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.2
    mql: |
      file("/etc/kubernetes/manifests/kube-apiserver.yaml") {
        user.name == "root"
        group.name == "root"
      }
    docs:
      desc: |-
        Ensure that the API server pod specification file ownership is set to `root:root`.

        **Rationale:**

        The API server pod specification file controls various parameters that set the behavior of the API server. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml
        ```

        Verify that the ownership is set to `root:root`.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chown root:root /etc/kubernetes/manifests/kube-apiserver.yaml
            ```
  - uid: cis-kubernetes--1.1.20
    title: Ensure that the Kubernetes PKI certificate file permissions are set to 600 or more restrictive
    impact: 75
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.20
    props:
      - uid: cis-kubernetes--1.1.20
        title: Define the possible locations of the file paths where the Kubernetes PKI certificate file are stored
        mql: |
          return
          [
            "/etc/kubernetes/ssl/",
            "/var/lib/minikube/certs/",
            "/etc/kubernetes/pki/",
          ]
    mql: |
      props.pkiCertificateFilePaths.any(file(_).exists)
      props.pkiCertificateFilePaths.where(file(_).exists) {
        files.find(from: _ , type: 'file', regex: '\(.*\.crt\)\|\(.*\.pem\)').where(basename != /key.*\.pem/) {
          path
          permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that Kubernetes PKI certificate files have permissions of `600` or more restrictive.

        **Rationale:**

        Kubernetes makes use of a number of certificate files as part of the operation of its components. The permissions on these files should be set to `600` or more restrictive to protect their integrity.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c '%a' /etc/kubernetes/pki/*.crt
        ```
        Verify that the permissions are `600` or more restrictive.

        or

        ```
        ls -l /etc/kubernetes/pki/*.crt
        ```
        Verify -rw------
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod -R 600 /etc/kubernetes/pki/*.crt
            ```
  - uid: cis-kubernetes--1.1.21
    title: Ensure that the Kubernetes PKI key file permissions are set to 600
    impact: 80
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.21
    props:
      - uid: cis-kubernetes--1.1.21
        title: Define the possible locations of the file paths where the Kubernetes PKI certificate file are stored
        mql: |
          return
          [
            "/etc/kubernetes/ssl/",
            "/var/lib/minikube/certs/",
            "/etc/kubernetes/pki/",
          ]
    mql: |
      props.pkiCertificateFilePaths.any(file(_).exists)
      props.pkiCertificateFilePaths.where(file(_).exists) {
        files.find(from: _ , type: 'file', regex: '\(.*\.key\)\|\(.*-key\.pem\)') {
          path
          permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that Kubernetes PKI key files have permissions of `600`.

        **Rationale:**

        Kubernetes makes use of a number of key files as part of the operation of its components. The permissions on these files should be set to `600` to protect their integrity and confidentiality.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c '%a' /etc/kubernetes/pki/*.key
        ```
        Verify that the permissions are `600` or more restrictive.

        or

        ```
        ls -l /etc/kubernetes/pki/*.key
        ```
        Verify -rw------
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod -R 600 /etc/kubernetes/pki/*.key
            ```
  - uid: cis-kubernetes--1.1.3
    title: Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive
    impact: 65
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.3
    props:
      - uid: cis-kubernetes--1.1.3
        title: Define possible locations of the controller manager pod specification file
        mql: |
          return [
          "/etc/kubernetes/manifests/kube-controller-manager.yaml",
          ]
    mql: |
      props.kubeControllerManager.any(file(_).exists)
      props.kubeControllerManager.where(file(_).exists) {
        file (_) {
          path
          permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that the controller manager pod specification file has permissions of `600` or more restrictive.

        **Rationale:**

        The controller manager pod specification file controls various parameters that set the behavior of the Controller Manager on the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %a /etc/kubernetes/manifests/kube-controller-manager.yaml
        ```

        Verify that the permissions are `600` or more restrictive.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod 600 /etc/kubernetes/manifests/kube-controller-manager.yaml
            ```
  - uid: cis-kubernetes--1.1.4
    title: Ensure that the controller manager pod specification file ownership is set to root:root
    impact: 65
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.4
    mql: |
      file("/etc/kubernetes/manifests/kube-controller-manager.yaml") {
        user.name == "root"
        group.name == "root"
      }
    docs:
      desc: |-
        Ensure that the controller manager pod specification file ownership is set to `root:root`.

        **Rationale:**

        The controller manager pod specification file controls various parameters that set the behavior of various components of the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml
        ```

        Verify that the ownership is set to `root:root`.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml
            ```
  - uid: cis-kubernetes--1.1.5
    title: Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive
    impact: 65
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.5
    props:
      - uid: cis-kubernetes--1.1.5
        title: Define the location of the scheduler pod specification file
        mql: |
          return [
                  "/etc/kubernetes/manifests/kube-scheduler.yaml",
          ]
    mql: |
      props.kubeSchedulerFile.any(file(_).exists)
      props.kubeSchedulerFile.where(file(_).exists) {
          file (_) {
          path
          permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that the scheduler pod specification file has permissions of `600` or more restrictive.

        **Rationale:**

        The scheduler pod specification file controls various parameters that set the behavior of the Scheduler service in the master node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %a /etc/kubernetes/manifests/kube-scheduler.yaml
        ```

        Verify that the permissions are `600` or more restrictive.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod 600 /etc/kubernetes/manifests/kube-scheduler.yaml
            ```
  - uid: cis-kubernetes--1.1.6
    title: Ensure that the scheduler pod specification file ownership is set to root:root
    impact: 65
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.6
    mql: |
      file("/etc/kubernetes/manifests/kube-scheduler.yaml") {
        user.name == "root"
        group.name == "root"
      }
    docs:
      desc: |-
        Ensure that the scheduler pod specification file ownership is set to `root:root`.

        **Rationale:**

        The scheduler pod specification file controls various parameters that set the behavior of the `kube-scheduler` service in the master node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml
        ```

        Verify that the ownership is set to `root:root`.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chown root:root /etc/kubernetes/manifests/kube-scheduler.yaml
            ```
  - uid: cis-kubernetes--1.1.7
    title: Ensure that the etcd pod specification file permissions are set to 600 or more restrictive
    impact: 80
    filters: processes.where(executable == /etcd/).list != empty
    tags:
      cisecurity.org/recommendation: 1.1.7
    props:
      - uid: cis-kubernetes--1.1.7
        title: Define the location of the etcd pod specification file
        mql: |
          return [
          "/etc/kubernetes/manifests/etcd.yaml",
          ]
    mql: |
      props.kubeEtcdPodSpecFile.any(file(_).exists)
      props.kubeEtcdPodSpecFile.where(file(_).exists) {
        file (_) {
          path
          permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that the `/etc/kubernetes/manifests/etcd.yaml` file has permissions of `600` or more restrictive.

        **Rationale:**

        The etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` controls various parameters that set the behavior of the `etcd` service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %a /etc/kubernetes/manifests/etcd.yaml
        ```

        Verify that the permissions are `600` or more restrictive.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod 600 /etc/kubernetes/manifests/etcd.yaml
            ```
  - uid: cis-kubernetes--1.1.8
    title: Ensure that the etcd pod specification file ownership is set to root:root
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.8
    mql: |
      file("/etc/kubernetes/manifests/etcd.yaml") {
        user.name == "root"
        group.name == "root"
      }
    docs:
      desc: |-
        Ensure that the `/etc/kubernetes/manifests/etcd.yaml` file ownership is set to `root:root`.

        **Rationale:**

        The etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` controls various parameters that set the behavior of the `etcd` service in the master node. etcd is a highly-available key-value store which Kubernetes uses for persistent storage of all of its REST API object. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %U:%G /etc/kubernetes/manifests/etcd.yaml
        ```

        Verify that the ownership is set to `root:root`.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chown root:root /etc/kubernetes/manifests/etcd.yaml
            ```
  - uid: cis-kubernetes--1.1.9
    title: Ensure that the Container Network Interface file permissions are set to 600 or more restrictive
    impact: 80
    filters: null
    tags:
      cisecurity.org/recommendation: 1.1.9
    props:
      - uid: cis-kubernetes--1.1.9
        title: Defines the possible locations of the Container Network Interface file
        mql: |
          return
          [
            "/etc/cni/net.d",
            kubelet.process.flags["cni-conf-dir"],
          ]
    mql: |
      props.containerNetworkInterfaceFile.any(file(_).exists)
      props.containerNetworkInterfaceFile.where(file(_).exists) {
        files.find(from: _ , type: 'file',).where(basename == /\.conf$|\.conflist$/) {
          path
          permissions {
            user_executable == false
            group_writeable == false
            group_executable == false
            group_readable == false
            other_writeable == false
            other_executable == false
            other_readable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that the Container Network Interface files have permissions of `600` or more restrictive.

        **Rationale:**

        Container Network Interface provides various networking options for overlay networking. You should consult their documentation and restrict their respective file permissions to maintain the integrity of those files. Those files should be writable by only the administrators on the system.

        **Impact:**

        None
      audit: |-
        Run the below command (based on the file location on your system) on the Control Plane node. For example,

        ```
        stat -c %a <path/to/cni/files>
        ```

        Verify that the permissions are `600` or more restrictive.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the Control Plane node. For example,

            ```
            chmod 600 <path/to/cni/files>
            ```
  - uid: cis-kubernetes--1.2.1
    title: Ensure that the --anonymous-auth argument is set to false
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.1
    variants:
      - uid: kubernetes-api-server-anonymous-auth-argument-false-apiserver
      - uid: kubernetes-api-server-anonymous-auth-argument-false-kubelet
    docs:
      desc: |-
        Disable anonymous requests to the API server.

        **Rationale:**

        When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the API server. You should rely on authentication to authorize access and disallow anonymous requests.

        If you are using RBAC authorization, it is generally considered reasonable to allow anonymous access to the API Server for health checks and discovery purposes, and hence this recommendation is not scored. However, you should consider whether anonymous discovery is an acceptable risk for your purposes.

        **Impact:**

        Anonymous requests will be rejected.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--anonymous-auth` argument is set to `false`.

        Alternative Audit

        kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[*]}{.spec.containers[*].command} {"\n"}{end}' | grep '\--anonymous-auth' | grep -i false

        If the exit code is '1', then the control isn't present / failed
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the below parameter.

            ```
            --anonymous-auth=false
            ```
  - uid: cis-kubernetes--1.2.10
    title: Ensure that the admission control plugin AlwaysAdmit is not set
    impact: 80
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.10
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["enable-admission-plugins"] != /AlwaysAdmit/
      )
    docs:
      desc: |-
        Do not allow all requests.

        **Rationale:**

        Setting admission control plugin `AlwaysAdmit` allows all requests and do not filter any requests.

        The `AlwaysAdmit` admission controller was deprecated in Kubernetes v1.13. Its behavior was equivalent to turning off all admission controllers.

        **Impact:**

        Only requests explicitly allowed by the admissions control plugins would be served.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that if the `--enable-admission-plugins` argument is set, its value does not include `AlwaysAdmit`.
      remediation:
        - desc: Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and either remove the `--enable-admission-plugins` parameter, or set it to a value that does not include `AlwaysAdmit`.
  - uid: cis-kubernetes--1.2.11
    title: Ensure that the admission control plugin AlwaysPullImages is set
    impact: 80
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.11
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["enable-admission-plugins"] == /AlwaysPullImages/
      )
    docs:
      desc: |-
        Always pull images.

        **Rationale:**

        Setting admission control policy to `AlwaysPullImages` forces every new pod to pull the required images every time. In a multi-tenant cluster users can be assured that their private images can only be used by those who have the credentials to pull them. Without this admission control policy, once an image has been pulled to a node, any pod from any user can use it simply by knowing the image’s name, without any authorization check against the image ownership. When this plug-in is enabled, images are always pulled prior to starting containers, which means valid credentials are required.

        **Impact:**

        Credentials would be required to pull the private images every time. Also, in trusted environments, this might increases load on network, registry, and decreases speed.

        This setting could impact offline or isolated clusters, which have images pre-loaded and do not have access to a registry to pull in-use images. This setting is not appropriate for clusters which use this configuration.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--enable-admission-plugins` argument is set to a value that includes `AlwaysPullImages`.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--enable-admission-plugins` parameter to include `AlwaysPullImages`.

            ```
            --enable-admission-plugins=...,AlwaysPullImages,...
            ```
  - uid: cis-kubernetes--1.2.15
    title: Ensure that the --profiling argument is set to false
    impact: 30
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.15
    variants:
      - uid: kubernetes-profiling-argument-false-apiserver
      - uid: kubernetes-profiling-argument-false-controller-manager
      - uid: kubernetes-profiling-argument-false-kube-scheduler
    docs:
      desc: |-
        Disable profiling, if not needed.

        **Rationale:**

        Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.

        **Impact:**

        Profiling information would not be available.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--profiling` argument is set to `false`.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the below parameter.

            ```
            --profiling=false
            ```
  - uid: cis-kubernetes--1.2.16
    title: Ensure that the --audit-log-path argument is set
    impact: 60
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.16
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["audit-log-path"] != empty
      )
    docs:
      desc: |-
        Enable auditing on the Kubernetes API Server and set the desired audit log path.

        **Rationale:**

        Auditing the Kubernetes API Server provides a security-relevant chronological set of records documenting the sequence of activities that have affected system by individual users, administrators or other components of the system. Even though currently, Kubernetes provides only basic audit capabilities, it should be enabled. You can enable it by setting an appropriate audit log path.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--audit-log-path` argument is set as appropriate.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--audit-log-path` parameter to a suitable path and file where you would like audit logs to be written, for example:

            ```
            --audit-log-path=/var/log/apiserver/audit.log
            ```
  - uid: cis-kubernetes--1.2.17
    title: Ensure that the --audit-log-maxage argument is set to 30 or as appropriate
    impact: 60
    filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty
    tags:
      cisecurity.org/recommendation: 1.2.17
    mql: |
      processes.where(executable == /kube-apiserver/).all(flags["audit-log-maxage"] != empty)
      processes.where(executable == /kube-apiserver/).all(flags["audit-log-maxage"] >= 30)
    docs:
      desc: |-
        Retain the logs for at least 30 days or as appropriate.

        **Rationale:**

        Retaining logs for at least 30 days ensures that you can go back in time and investigate or correlate any events. Set your audit log retention period to 30 days or as per your business requirements.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--audit-log-maxage` argument is set to `30` or as appropriate.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--audit-log-maxage` parameter to 30 or as an appropriate number of days:

            ```
            --audit-log-maxage=30
            ```
  - uid: cis-kubernetes--1.2.18
    title: Ensure that the --audit-log-maxbackup argument is set to 10 or as appropriate
    impact: 60
    filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty
    tags:
      cisecurity.org/recommendation: 1.2.18
    mql: |
      processes.where(executable == /kube-apiserver/).all(flags["audit-log-maxbackup"] != empty)
      processes.where(executable == /kube-apiserver/).all(flags["audit-log-maxbackup"] >= 10)
    docs:
      desc: |-
        Retain 10 or an appropriate number of old log files.

        **Rationale:**

        Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. For example, if you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--audit-log-maxbackup` argument is set to `10` or as appropriate.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--audit-log-maxbackup` parameter to 10 or to an appropriate value.

            ```
            --audit-log-maxbackup=10
            ```
  - uid: cis-kubernetes--1.2.19
    title: Ensure that the --audit-log-maxsize argument is set to 100 or as appropriate
    impact: 60
    filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty
    tags:
      cisecurity.org/recommendation: 1.2.19
    mql: |
      processes.where(executable == /kube-apiserver/).all(flags["audit-log-maxsize"] != empty)
      processes.where(executable == /kube-apiserver/).all(flags["audit-log-maxsize"] >= 100)
    docs:
      desc: |-
        Rotate log files on reaching 100 MB or as appropriate.

        **Rationale:**

        Kubernetes automatically rotates the log files. Retaining old log files ensures that you would have sufficient log data available for carrying out any investigation or correlation. If you have set file size of 100 MB and the number of old log files to keep as 10, you would approximate have 1 GB of log data that you could potentially use for your analysis.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--audit-log-maxsize` argument is set to `100` or as appropriate.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--audit-log-maxsize` parameter to an appropriate size in MB. For example, to set it as 100 MB:

            ```
            --audit-log-maxsize=100
            ```
  - uid: cis-kubernetes--1.2.2
    title: Ensure that the --token-auth-file parameter is not set
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.2
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["token-auth-file"] == empty
      )
    docs:
      desc: |-
        Do not use token based authentication.

        **Rationale:**

        The token-based authentication utilizes static tokens to authenticate requests to the apiserver. The tokens are stored in clear-text in a file on the apiserver, and cannot be revoked or rotated without restarting the apiserver. Hence, do not use static token-based authentication.

        **Impact:**

        You will have to configure and use alternate authentication mechanisms such as certificates. Static token based authentication could not be used.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--token-auth-file` argument does not exist.

        Alternative Audit Method

        kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[*]}{.spec.containers[*].command} {"\n"}{end}' | grep '\--token-auth-file' | grep -i false

        If the exit code is '1', then the control isn't present / failed
      remediation:
        - desc: Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and remove the `--token-auth-file=<filename>` parameter.
  - uid: cis-kubernetes--1.2.20
    title: Ensure that the --request-timeout argument is set as appropriate
    impact: 70
    filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty
    tags:
      cisecurity.org/recommendation: 1.2.20
    mql: |
      processes.where(executable == /kube-apiserver/).all(flags["request-timeout"] == empty)
        || processes.where(executable == /kube-apiserver/).all(flags["request-timeout"] >= 60)
      && processes.where(executable == /kube-apiserver/).all(flags["request-timeout"] <= 300)
    docs:
      desc: |-
        Set global request timeout for API server requests as appropriate.

        **Rationale:**

        Setting global request timeout allows extending the API server request timeout limit to a duration appropriate to the user's connection speed. By default, it is set to 60 seconds which might be problematic on slower connections making cluster resources inaccessible once the data volume for requests exceeds what can be transmitted in 60 seconds. But, setting this timeout limit to be too large can exhaust the API server resources making it prone to Denial-of-Service attack. Hence, it is recommended to set this limit as appropriate and change the default limit of 60 seconds only if needed.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--request-timeout` argument is either not set or set to an appropriate value.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` and set the below parameter as appropriate and if needed. For example,

            ```
            --request-timeout=300s
            ```
  - uid: cis-kubernetes--1.2.21
    title: Ensure that the --service-account-lookup argument is set to true
    impact: 80
    filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty
    tags:
      cisecurity.org/recommendation: 1.2.21
    mql: |
      processes.where(executable == /kube-apiserver/ && flags["service-account-lookup"]  != empty).all(
        flags["service-account-lookup"] == "true"
      )
    docs:
      desc: |-
        Validate service account before validating token.

        **Rationale:**

        If `--service-account-lookup` is not enabled, the apiserver only verifies that the authentication token is valid, and does not validate that the service account token mentioned in the request is actually present in etcd. This allows using a service account token even after the corresponding service account is deleted. This is an example of time of check to time of use security issue.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that if the `--service-account-lookup` argument exists it is set to `true`.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the below parameter.

            ```
            --service-account-lookup=true
            ```

            Alternatively, you can delete the `--service-account-lookup` parameter from this file so that the default takes effect.
  - uid: cis-kubernetes--1.2.22
    title: Ensure that the --service-account-key-file argument is set as appropriate
    impact: 80
    filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty
    tags:
      cisecurity.org/recommendation: 1.2.22
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["service-account-key-file"] != empty
      )
    docs:
      desc: |-
        Explicitly set a service account public key file for service accounts on the apiserver.

        **Rationale:**

        By default, if no `--service-account-key-file` is specified to the apiserver, it uses the private key from the TLS serving certificate to verify service account tokens. To ensure that the keys for service account tokens could be rotated as needed, a separate public/private key pair should be used for signing service account tokens. Hence, the public key should be specified to the apiserver with `--service-account-key-file`.

        **Impact:**

        The corresponding private key must be provided to the controller manager. You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--service-account-key-file` argument exists and is set as appropriate.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--service-account-key-file` parameter to the public key file for service accounts:

            ```
            --service-account-key-file=<filename>
            ```
  - uid: cis-kubernetes--1.2.23
    title: Ensure that the --etcd-certfile and --etcd-keyfile arguments are set as appropriate
    impact: 100
    filters: asset.family.contains('linux') && processes.where( executable == /kube-apiserver/ ).list != empty
    tags:
      cisecurity.org/recommendation: 1.2.23
    mql: |
      processes.where(executable == /kube-apiserver/).all(flags["etcd-certfile"] != empty)
      processes.where(executable == /kube-apiserver/).all(flags["etcd-keyfile"] != empty)
    docs:
      desc: |-
        etcd should be configured to make use of TLS encryption for client connections.

        **Rationale:**

        etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a client certificate and key.

        **Impact:**

        TLS and client certificate authentication must be configured for etcd.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--etcd-certfile` and `--etcd-keyfile` arguments exist and they are set as appropriate.
      remediation:
        - desc: |-
            Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the etcd certificate and key file parameters.

            ```
            --etcd-certfile=<path/to/client-certificate-file>
            --etcd-keyfile=<path/to/client-key-file>
            ```
  - uid: cis-kubernetes--1.2.24
    title: Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate
    impact: 90
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.24
    variants:
      - uid: kubernetes-ensure-tls-cert-file-tls-private-key-file-arguments-are-appropriate-apiserver
      - uid: kubernetes-ensure-tls-cert-file-tls-private-key-file-arguments-are-appropriate-kubelet
    docs:
      desc: |-
        Setup TLS connection on the API server.

        **Rationale:**

        API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic.

        **Impact:**

        TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--tls-cert-file` and `--tls-private-key-file` arguments exist and they are set as appropriate.
      remediation:
        - desc: |-
            Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the TLS certificate and private key file parameters.

            ```
            --tls-cert-file=<path/to/tls-certificate-file>
            --tls-private-key-file=<path/to/tls-key-file>
            ```
  - uid: cis-kubernetes--1.2.25
    title: Ensure that the --client-ca-file argument is set as appropriate
    impact: 90
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.25
    variants:
      - uid: kubernetes-kubelet-client-ca-file-argument-appropriate-apiserver
      - uid: kubernetes-kubelet-client-ca-file-argument-appropriate-kubelet
    docs:
      desc: |-
        Setup TLS connection on the API server.

        **Rationale:**

        API server communication contains sensitive parameters that should remain encrypted in transit. Configure the API server to serve only HTTPS traffic. If `--client-ca-file` argument is set, any request presenting a client certificate signed by one of the authorities in the `client-ca-file` is authenticated with an identity corresponding to the CommonName of the client certificate.

        **Impact:**

        TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--client-ca-file` argument exists and it is set as appropriate.
      remediation:
        - desc: |-
            Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the client certificate authority file.

            ```
            --client-ca-file=<path/to/client-ca-file>
            ```
  - uid: cis-kubernetes--1.2.26
    title: Ensure that the --etcd-cafile argument is set as appropriate
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.26
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["etcd-cafile"] != empty
      )
    docs:
      desc: |-
        etcd should be configured to make use of TLS encryption for client connections.

        **Rationale:**

        etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be protected by client authentication. This requires the API server to identify itself to the etcd server using a SSL Certificate Authority file.

        **Impact:**

        TLS and client certificate authentication must be configured for etcd.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--etcd-cafile` argument exists and it is set as appropriate.
      remediation:
        - desc: |-
            Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the etcd certificate authority file parameter.

            ```
            --etcd-cafile=<path/to/ca-file>
            ```
  - uid: cis-kubernetes--1.2.27
    title: Ensure that the --encryption-provider-config argument is set as appropriate
    impact: 75
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.27
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["encryption-provider-config"] != empty
      )
    docs:
      desc: |-
        Encrypt etcd key-value store.

        **Rationale:**

        etcd is a highly available key-value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted at rest to avoid any disclosures.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--encryption-provider-config` argument is set to a `EncryptionConfig` file. Additionally, ensure that the `EncryptionConfig` file has all the desired `resources` covered especially any secrets.
      remediation:
        - desc: |-
            Follow the Kubernetes documentation and configure a `EncryptionConfig` file. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the `--encryption-provider-config` parameter to the path of that file:

            ```
            --encryption-provider-config=</path/to/EncryptionConfig/File>
            ```
  - uid: cis-kubernetes--1.2.28
    title: Ensure that encryption providers are appropriately configured
    impact: 80
    filters: processes.where(executable == /kube-apiserver/).any(flags['encryption-provider-config']) != false
    tags:
      cisecurity.org/recommendation: 1.2.28
    mql: |-
      encfile = processes.where(executable == /kube-apiserver/).list.map(flags["encryption-provider-config"])
      encfile.first != empty
      parse.yaml(encfile.first).params['resources'].first['providers'].first['aescbc'].length > 0 ||
      parse.yaml(encfile.first).params['resources'].first['providers'].first['kms'].length > 0 ||
      parse.yaml(encfile.first).params['resources'].first['providers'].first['secretbox'].length > 0
    docs:
      desc: |-
        Where `etcd` encryption is used, appropriate providers should be configured.

        **Rationale:**

        Where `etcd` encryption is used, it is important to ensure that the appropriate set of encryption providers is used. Currently, the `aescbc`, `kms` and `secretbox` are likely to be appropriate options.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Get the `EncryptionConfig` file set for `--encryption-provider-config` argument. Verify that `aescbc`, `kms` or `secretbox` is set as the encryption provider for all the desired `resources`.
      remediation:
        - desc: Follow the Kubernetes documentation and configure a `EncryptionConfig` file. In this file, choose `aescbc`, `kms` or `secretbox` as the encryption provider.
  - uid: cis-kubernetes--1.2.29
    title: Ensure that the API Server only makes use of Strong Cryptographic Ciphers
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.29
    props:
      - uid: cis-kubernetes--1.2.29
        title: Define the hardened SSL/ TLS ciphers
        mql: |
          return ["TLS_AES_128_GCM_SHA256", "TLS_AES_256_GCM_SHA384", "TLS_CHACHA20_POLY1305_SHA256",
          "TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256", "TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA",
          "TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384", "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305",
          "TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256", "TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA",
          "TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256",
          "TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA", "TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384",
          "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305", "TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256",
          "TLS_RSA_WITH_3DES_EDE_CBC_SHA", "TLS_RSA_WITH_AES_128_CBC_SHA", "TLS_RSA_WITH_AES_128_GCM_SHA256",
          "TLS_RSA_WITH_AES_256_CBC_SHA", "TLS_RSA_WITH_AES_256_GCM_SHA384"]
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["tls-cipher-suites"].split(",").map(_.trim).containsOnly(props.allowedCiphers)
      )
    docs:
      desc: |-
        Ensure that the API server is configured to only use strong cryptographic ciphers.

        **Rationale:**

        TLS ciphers have had a number of known vulnerabilities and weaknesses, which can reduce the protection provided by them. By default Kubernetes supports a number of TLS ciphersuites including some that have security concerns, weakening the protection provided.

        **Impact:**

        API server clients that cannot support modern cryptographic ciphers will not be able to make connections to the API server.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--tls-cipher-suites` argument is set as outlined in the remediation procedure below.
      remediation:
        - desc: |-
            Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the below parameter.

            ```
            --tls-cipher-suites=TLS_AES_128_GCM_SHA256,TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256,TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256,TLS_RSA_WITH_3DES_EDE_CBC_SHA,TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384.
            ```
  - uid: cis-kubernetes--1.2.3
    title: Ensure that the DenyServiceExternalIPs is set
    impact: 50
    filters: processes.where(executable == /kube-apiserver/).list != empty
    tags:
      cisecurity.org/recommendation: 1.2.3
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["enable-admission-plugins"] != /DenyServiceExternalIPs/
      )
    docs:
      desc: |-
        This admission controller rejects all net-new usage of the Service field externalIPs.

        **Rationale:**

        Most users do not need the ability to set the `externalIPs` field for a `Service` at all, and cluster admins should consider disabling this functionality by enabling the `DenyServiceExternalIPs` admission controller. Clusters that do need to allow this functionality should consider using some custom policy to manage its usage.

        **Impact:**

        When enabled, users of the cluster may not create new Services which use externalIPs and may not add new values to externalIPs on existing Service objects.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `DenyServiceExternalIPs' argument exist as a string value in --disable-admission-plugins.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and remove the `--DenyServiceExternalIPs'parameter

            or

            The Kubernetes API server flag disable-admission-plugins takes a comma-delimited list of admission control plugins to be disabled, even if they are in the list of plugins enabled by default.

            `kube-apiserver --disable-admission-plugins=DenyServiceExternalIPs,AlwaysDeny ...`
  - uid: cis-kubernetes--1.2.4
    title: Ensure that the --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate
    impact: 90
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.4
    mql: |
      processes.where(executable == /kube-apiserver/).all(flags["kubelet-client-certificate"] != empty)
      processes.where(executable == /kube-apiserver/).all(flags["kubelet-client-key"] != empty)
    docs:
      desc: |-
        Enable certificate based kubelet authentication.

        **Rationale:**

        The apiserver, by default, does not authenticate itself to the kubelet's HTTPS endpoints. The requests from the apiserver are treated anonymously. You should set up certificate-based kubelet authentication to ensure that the apiserver authenticates itself to kubelets when submitting requests.

        **Impact:**

        You require TLS to be configured on apiserver as well as kubelets.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--kubelet-client-certificate` and `--kubelet-client-key` arguments exist and they are set as appropriate.

        Alternative Audit

        kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {"\n"}{end}' | grep '--kubelet-client-certificate' | grep -i false

        If the exit code is '1', then the control isn't present / failed
      remediation:
        - desc: |-
            Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the kubelet client certificate and key parameters as below.

            ```
            --kubelet-client-certificate=<path/to/client-certificate-file>
            --kubelet-client-key=<path/to/client-key-file>
            ```
  - uid: cis-kubernetes--1.2.5
    title: Ensure that the --kubelet-certificate-authority argument is set as appropriate
    impact: 90
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.5
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["kubelet-certificate-authority"] != empty
      )
    docs:
      desc: |-
        Verify kubelet's certificate before establishing connection.

        **Rationale:**

        The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.

        **Impact:**

        You require TLS to be configured on apiserver as well as kubelets.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--kubelet-certificate-authority` argument exists and is set as appropriate.

        Alternative Audit

        kubectl get pod -nkube-system -lcomponent=kube-apiserver -o=jsonpath='{range .items[]}{.spec.containers[].command} {"\n"}{end}' | grep '--kubelet-certificate-Authority' | grep -i false

        If the exit code is '1', then the control isn't present / failed
      remediation:
        - desc: |-
            Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--kubelet-certificate-authority` parameter to the path to the cert file for the certificate authority.

            ```
            --kubelet-certificate-authority=<ca-string>
            ```
  - uid: cis-kubernetes--1.2.6
    title: Ensure that the --authorization-mode argument is not set to AlwaysAllow
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.6
    variants:
      - uid: kubernetes-api-server-authorization-mode-argument-not-alwaysallow-apiserver
      - uid: kubernetes-api-server-authorization-mode-argument-not-alwaysallow-kubelet
      - uid: kubernetes-api-server-authorization-mode-argument-not-alwaysallow-kubelet-aks
      - uid: kubernetes-api-server-authorization-mode-argument-not-alwaysallow-kubelet-eks
    docs:
      desc: |-
        Do not always authorize all requests.

        **Rationale:**

        The API Server, can be configured to allow all requests. This mode should not be used on any production cluster.

        **Impact:**

        Only authorized requests will be served.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--authorization-mode` argument exists and is not set to `AlwaysAllow`.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--authorization-mode` parameter to values other than `AlwaysAllow`. One such example could be as below.

            ```
            --authorization-mode=RBAC
            ```
  - uid: cis-kubernetes--1.2.7
    title: Ensure that the --authorization-mode argument includes Node
    impact: 50
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.7
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["authorization-mode"] == /Node/
      )
    docs:
      desc: |-
        Restrict kubelet nodes to reading only objects associated with them.

        **Rationale:**

        The `Node` authorization mode only allows kubelets to read `Secret`, `ConfigMap`, `PersistentVolume`, and `PersistentVolumeClaim` objects associated with their nodes.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--authorization-mode` argument exists and is set to a value to include `Node`.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--authorization-mode` parameter to a value that includes `Node`.

            ```
            --authorization-mode=Node,RBAC
            ```
  - uid: cis-kubernetes--1.2.8
    title: Ensure that the --authorization-mode argument includes RBAC
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.8
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["authorization-mode"] == /RBAC/
      )
    docs:
      desc: |-
        Turn on Role Based Access Control.

        **Rationale:**

        Role Based Access Control (RBAC) allows fine-grained control over the operations that different entities can perform on different objects in the cluster. It is recommended to use the RBAC authorization mode.

        **Impact:**

        When RBAC is enabled you will need to ensure that appropriate RBAC settings (including Roles, RoleBindings and ClusterRoleBindings) are configured to allow appropriate access.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--authorization-mode` argument exists and is set to a value to include `RBAC`.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--authorization-mode` parameter to a value that includes `RBAC`, for example:

            ```
            --authorization-mode=Node,RBAC
            ```
  - uid: cis-kubernetes--1.2.9
    title: Ensure that the admission control plugin EventRateLimit is set
    impact: 60
    filters: null
    tags:
      cisecurity.org/recommendation: 1.2.9
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["enable-admission-plugins"] == /EventRateLimit/
      )
    docs:
      desc: |-
        Limit the rate at which the API server accepts requests.

        **Rationale:**

        Using `EventRateLimit` admission control enforces a limit on the number of events that the API Server will accept in a given time slice. A misbehaving workload could overwhelm and DoS the API Server, making it unavailable. This particularly applies to a multi-tenant cluster, where there might be a small percentage of misbehaving tenants which could have a significant impact on the performance of the cluster overall. Hence, it is recommended to limit the rate of events that the API server will accept.

        **Impact:**

        You need to carefully tune in limits as per your environment.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--enable-admission-plugins` argument is set to a value that includes `EventRateLimit`.
      remediation:
        - desc: |-
            Follow the Kubernetes documentation and set the desired limits in a configuration file.

            Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` and set the below parameters.

            ```
            --enable-admission-plugins=...,EventRateLimit,...
            --admission-control-config-file=<path/to/configuration/file>
            ```
  - uid: cis-kubernetes--1.3.1
    title: Ensure that the --terminated-pod-gc-threshold argument is set as appropriate
    impact: 40
    filters: null
    tags:
      cisecurity.org/recommendation: 1.3.1
    mql: |
      processes.where(executable == /kube-controller-manager/).all(
        flags["terminated-pod-gc-threshold"] != empty
      )
    docs:
      desc: |-
        Activate garbage collector on pod termination, as appropriate.

        **Rationale:**

        Garbage collection is important to ensure sufficient resource availability and avoiding degraded performance and availability. In the worst case, the system might crash or just be unusable for a long period of time. The current setting for garbage collection is 12,500 terminated pods which might be too high for your system to sustain. Based on your system resources and tests, choose an appropriate threshold value to activate garbage collection.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-controller-manager
        ```

        Verify that the `--terminated-pod-gc-threshold` argument is set as appropriate.
      remediation:
        - desc: |-
            Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the `--terminated-pod-gc-threshold` to an appropriate threshold, for example:

            ```
            --terminated-pod-gc-threshold=10
            ```
  - uid: cis-kubernetes--1.3.2
    title: Ensure that the --profiling argument is set to false
    impact: 30
    filters: null
    tags:
      cisecurity.org/recommendation: 1.3.2
    variants:
      - uid: kubernetes-profiling-argument-false-apiserver
      - uid: kubernetes-profiling-argument-false-controller-manager
      - uid: kubernetes-profiling-argument-false-kube-scheduler
    docs:
      desc: |-
        Disable profiling, if not needed.

        **Rationale:**

        Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.

        **Impact:**

        Profiling information would not be available.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-controller-manager
        ```

        Verify that the `--profiling` argument is set to `false`.
      remediation:
        - desc: |-
            Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the below parameter.

            ```
            --profiling=false
            ```
  - uid: cis-kubernetes--1.3.3
    title: Ensure that the --use-service-account-credentials argument is set to true
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: 1.3.3
    mql: |
      processes.where(executable == /kube-controller-manager/).all(
        flags["use-service-account-credentials"] == "true"
      )
    docs:
      desc: |-
        Use individual service account credentials for each controller.

        **Rationale:**

        The controller manager creates a service account per controller in the `kube-system` namespace, generates a credential for it, and builds a dedicated API client with that service account credential for each controller loop to use. Setting the `--use-service-account-credentials` to `true` runs each control loop within the controller manager using a separate service account credential. When used in combination with RBAC, this ensures that the control loops run with the minimum permissions required to perform their intended tasks.

        **Impact:**

        Whatever authorizer is configured for the cluster, it must grant sufficient permissions to the service accounts to perform their intended tasks. When using the RBAC authorizer, those roles are created and bound to the appropriate service accounts in the `kube-system` namespace automatically with default roles and rolebindings that are auto-reconciled on startup.

        If using other authorization methods (ABAC, Webhook, etc), the cluster deployer is responsible for granting appropriate permissions to the service accounts (the required permissions can be seen by inspecting the `controller-roles.yaml` and `controller-role-bindings.yaml` files for the RBAC roles.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-controller-manager
        ```

        Verify that the `--use-service-account-credentials` argument is set to `true`.
      remediation:
        - desc: |-
            Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node to set the below parameter.

            ```
            --use-service-account-credentials=true
            ```
  - uid: cis-kubernetes--1.3.4
    title: Ensure that the --service-account-private-key-file argument is set as appropriate
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: 1.3.4
    mql: |
      processes.where(executable == /kube-controller-manager/).all(
        flags["service-account-private-key-file"] != empty
      )
    docs:
      desc: |-
        Explicitly set a service account private key file for service accounts on the controller manager.

        **Rationale:**

        To ensure that keys for service account tokens can be rotated as needed, a separate public/private key pair should be used for signing service account tokens. The private key should be specified to the controller manager with `--service-account-private-key-file` as appropriate.

        **Impact:**

        You would need to securely maintain the key file and rotate the keys based on your organization's key rotation policy.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-controller-manager
        ```

        Verify that the `--service-account-private-key-file` argument is set as appropriate.
      remediation:
        - desc: |-
            Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the `--service-account-private-key-file` parameter to the private key file for service accounts.

            ```
            --service-account-private-key-file=<filename>
            ```
  - uid: cis-kubernetes--1.3.5
    title: Ensure that the --root-ca-file argument is set as appropriate
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 1.3.5
    mql: |
      processes.where(executable == /kube-controller-manager/).all(
        flags["root-ca-file"] != empty
      )
    docs:
      desc: |-
        Allow pods to verify the API server's serving certificate before establishing connections.

        **Rationale:**

        Processes running within pods that need to contact the API server must verify the API server's serving certificate. Failing to do so could be a subject to man-in-the-middle attacks.

        Providing the root certificate for the API server's serving certificate to the controller manager with the `--root-ca-file` argument allows the controller manager to inject the trusted bundle into pods so that they can verify TLS connections to the API server.

        **Impact:**

        You need to setup and maintain root certificate authority file.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-controller-manager
        ```

        Verify that the `--root-ca-file` argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate.
      remediation:
        - desc: |-
            Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the `--root-ca-file` parameter to the certificate bundle file`.

            ```
            --root-ca-file=<path/to/file>
            ```
  - uid: cis-kubernetes--1.3.6
    title: Ensure that the RotateKubeletServerCertificate argument is set to true
    impact: 60
    filters: null
    tags:
      cisecurity.org/recommendation: 1.3.6
    mql: |
      processes.where(executable == /kube-controller-manager/).all(
        flags["feature-gates"] == "RotateKubeletServerCertificate=true"
      )
    docs:
      desc: |-
        Enable kubelet server certificate rotation on controller-manager.

        **Rationale:**

        `RotateKubeletServerCertificate` causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA security triad.

        Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to take care of rotation yourself.

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-controller-manager
        ```

        Verify that `RotateKubeletServerCertificate` argument exists and is set to `true`.
      remediation:
        - desc: |-
            Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the `--feature-gates` parameter to include `RotateKubeletServerCertificate=true`.

            ```
            --feature-gates=RotateKubeletServerCertificate=true
            ```
  - uid: cis-kubernetes--1.3.7
    title: Ensure that the --bind-address argument is set to 127.0.0.1
    impact: 65
    filters: null
    tags:
      cisecurity.org/recommendation: 1.3.7
    variants:
      - uid: kubernetes-scheduler-bind-address-argument-127-0-0-1-controller-manager
      - uid: kubernetes-scheduler-bind-address-argument-127-0-0-1-kube-scheduler
    docs:
      desc: |-
        Do not bind the Controller Manager service to non-loopback insecure addresses.

        **Rationale:**

        The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-controller-manager
        ```

        Verify that the `--bind-address` argument is set to 127.0.0.1
      remediation:
        - desc: Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and ensure the correct value for the `--bind-address` parameter
  - uid: cis-kubernetes--1.4.1
    title: Ensure that the --profiling argument is set to false
    impact: 30
    filters: null
    tags:
      cisecurity.org/recommendation: 1.4.1
    variants:
      - uid: kubernetes-profiling-argument-false-apiserver
      - uid: kubernetes-profiling-argument-false-controller-manager
      - uid: kubernetes-profiling-argument-false-kube-scheduler
    docs:
      desc: |-
        Disable profiling, if not needed.

        **Rationale:**

        Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.

        **Impact:**

        Profiling information would not be available.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-scheduler
        ```

        Verify that the `--profiling` argument is set to `false`.
      remediation:
        - desc: |-
            Edit the Scheduler pod specification file `/etc/kubernetes/manifests/kube-scheduler.yaml` file on the Control Plane node and set the below parameter.

            ```
            --profiling=false
            ```
  - uid: cis-kubernetes--1.4.2
    title: Ensure that the --bind-address argument is set to 127.0.0.1
    impact: 65
    filters: null
    tags:
      cisecurity.org/recommendation: 1.4.2
    variants:
      - uid: kubernetes-scheduler-bind-address-argument-127-0-0-1-controller-manager
      - uid: kubernetes-scheduler-bind-address-argument-127-0-0-1-kube-scheduler
    docs:
      desc: |-
        Do not bind the scheduler service to non-loopback insecure addresses.

        **Rationale:**

        The Scheduler API service which runs on port 10251/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-scheduler
        ```

        Verify that the `--bind-address` argument is set to 127.0.0.1
      remediation:
        - desc: Edit the Scheduler pod specification file `/etc/kubernetes/manifests/kube-scheduler.yaml` on the Control Plane node and ensure the correct value for the `--bind-address` parameter
  - uid: cis-kubernetes--2.1
    title: Ensure that the --cert-file and --key-file arguments are set as appropriate
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: "2.1"
    mql: |
      processes.where(executable == /etcd/).all(flags["cert-file"] != empty)
      processes.where(executable == /etcd/).all(flags["key-file"] != empty)
    docs:
      desc: |-
        Configure TLS encryption for the etcd service.

        **Rationale:**

        etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit.

        **Impact:**

        Client connections only over TLS would be served.
      audit: |-
        Run the following command on the etcd server node

        ```
        ps -ef | grep etcd
        ```

        Verify that the `--cert-file` and the `--key-file` arguments are set as appropriate.
      remediation:
        - desc: |-
            Follow the etcd service documentation and configure TLS encryption.

            Then, edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and set the below parameters.

            ```
            --cert-file=</path/to/ca-file>
            --key-file=</path/to/key-file>
            ```
  - uid: cis-kubernetes--2.2
    title: Ensure that the --client-cert-auth argument is set to true
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: "2.2"
    mql: |
      processes.where(executable == /etcd/).all(
        flags["client-cert-auth"] == "true"
      )
    docs:
      desc: |-
        Enable client authentication on etcd service.

        **Rationale:**

        etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service.

        **Impact:**

        All clients attempting to access the etcd server will require a valid client certificate.
      audit: |-
        Run the following command on the etcd server node:

        ```
        ps -ef | grep etcd
        ```

        Verify that the `--client-cert-auth` argument is set to `true`.
      remediation:
        - desc: |-
            Edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and set the below parameter.

            ```
            --client-cert-auth="true"
            ```
  - uid: cis-kubernetes--2.3
    title: Ensure that the --auto-tls argument is not set to true
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: "2.3"
    mql: |
      processes.where(executable == /etcd/).all(
        flags["auto-tls"] != "true"
      )
    docs:
      desc: |-
        Do not use self-signed certificates for TLS.

        **Rationale:**

        etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should not be available to unauthenticated clients. You should enable the client authentication via valid certificates to secure the access to the etcd service.

        **Impact:**

        Clients will not be able to use self-signed certificates for TLS.
      audit: |-
        Run the following command on the etcd server node:

        ```
        ps -ef | grep etcd
        ```

        Verify that if the `--auto-tls` argument exists, it is not set to `true`.
      remediation:
        - desc: |-
            Edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and either remove the `--auto-tls` parameter or set it to `false`.

            ```
            --auto-tls=false
            ```
  - uid: cis-kubernetes--2.4
    title: Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: "2.4"
    mql: |
      processes.where(executable == /etcd/).all(flags["peer-cert-file"] != empty)
      processes.where(executable == /etcd/).all(flags["peer-key-file"] != empty)
    docs:
      desc: |-
        etcd should be configured to make use of TLS encryption for peer connections.

        **Rationale:**

        etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be encrypted in transit and also amongst peers in the etcd clusters.

        **Impact:**

        etcd cluster peers would need to set up TLS for their communication.
      audit: |-
        Run the following command on the etcd server node:

        ```
        ps -ef | grep etcd
        ```

        Verify that the `--peer-cert-file` and `--peer-key-file` arguments are set as appropriate.

        **Note:** This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.
      remediation:
        - desc: |-
            Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster.

            Then, edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and set the below parameters.

            ```
            --peer-client-file=</path/to/peer-cert-file>
            --peer-key-file=</path/to/peer-key-file>
            ```
  - uid: cis-kubernetes--2.5
    title: Ensure that the --peer-client-cert-auth argument is set to true
    impact: 100
    filters: null
    tags:
      cisecurity.org/recommendation: "2.5"
    mql: |
      processes.where(executable == /etcd/).all(
        flags["peer-client-cert-auth"] == "true"
      )
    docs:
      desc: |-
        etcd should be configured for peer authentication.

        **Rationale:**

        etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster.

        **Impact:**

        All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.
      audit: |-
        Run the following command on the etcd server node:

        ```
        ps -ef | grep etcd
        ```

        Verify that the `--peer-client-cert-auth` argument is set to `true`.

        **Note:** This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.
      remediation:
        - desc: |-
            Edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and set the below parameter.

            ```
            --peer-client-cert-auth=true
            ```
  - uid: cis-kubernetes--2.6
    title: Ensure that the --peer-auto-tls argument is not set to true
    impact: 80
    filters: null
    tags:
      cisecurity.org/recommendation: "2.6"
    mql: |
      processes.where(executable == /etcd/).all(flags["peer-auto-tls"] == empty) ||
      processes.where(executable == /etcd/).all(flags["peer-auto-tls"] == "false")
    docs:
      desc: |-
        Do not use automatically generated self-signed certificates for TLS connections between peers.

        **Rationale:**

        etcd is a highly-available key value store used by Kubernetes deployments for persistent storage of all of its REST API objects. These objects are sensitive in nature and should be accessible only by authenticated etcd peers in the etcd cluster. Hence, do not use self-signed certificates for authentication.

        **Impact:**

        All peers attempting to communicate with the etcd server will require a valid client certificate for authentication.
      audit: |-
        Run the following command on the etcd server node:

        ```
        ps -ef | grep etcd
        ```

        Verify that if the `--peer-auto-tls` argument exists, it is not set to `true`.
        **Note:** This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.
      remediation:
        - desc: |-
            Edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and either remove the `--peer-auto-tls` parameter or set it to `false`.

            ```
            --peer-auto-tls=false
            ```
  - uid: cis-kubernetes--3.2.1
    title: Ensure that a minimal audit policy is created
    impact: 60
    filters: null
    tags:
      cisecurity.org/recommendation: 3.2.1
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["audit-policy-file"] != empty
      )
    docs:
      desc: |-
        Kubernetes can audit the details of requests made to the API server. The `--audit-policy-file` flag must be set for this logging to be enabled.

        **Rationale:**

        Logging is an important detective control for all systems, to detect potential unauthorised access.

        **Impact:**

        Audit logs will be created on the master nodes, which will consume disk space. Care should be taken to avoid generating too large volumes of log information as this could impact the available of the cluster nodes.
      audit: |-
        Run the following command on one of the cluster master nodes:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--audit-policy-file` is set. Review the contents of the file specified and ensure that it contains a valid audit policy.
      remediation:
        - desc: Create an audit policy file for your cluster.
  - uid: cis-kubernetes--4.1.1
    title: Ensure that the kubelet service file permissions are set to 600 or more restrictive
    impact: 60
    filters: null
    tags:
      cisecurity.org/recommendation: 4.1.1
    props:
      - uid: cis-kubernetes--4.1.1
        title: Define the possible locations of the kubelet service file
        mql: |
          return [
          "/etc/systemd/system/kubelet.service.d/kubeadm.conf",
          ]
    mql: |
      props.kubeletServiceFile.any(file(_).exists)
      props.kubeletServiceFile.where(file(_).exists) {
        file (_) {
          path
          permissions {
            user_readable == true
            user_executable == false
            group_readable == false
            group_writeable == false
            group_executable == false
            other_readable == false
            other_writeable == false
            other_executable == false
          }
        }
      }
    docs:
      desc: |-
        Ensure that the `kubelet` service file has permissions of `600` or more restrictive.

        **Rationale:**

        The `kubelet` service file controls various parameters that set the behavior of the `kubelet` service in the worker node. You should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.

        **Impact:**

        None
      audit: |-
        Automated AAC auditing has been modified to allow CIS-CAT to input a variable for the <PATH>/<FILENAME> of the kubelet service config file.

        Please set $kubelet_service_config=<PATH><filename> based on the file location on your system

        for example:

        ```
        export kubelet_service_config=/etc/systemd/system/kubelet.service.d/kubeadm.conf
        ```

        To perform the audit manually:
        Run the below command (based on the file location on your system) on the each worker node. For example,

        ```
        stat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
        ```

        Verify that the permissions are `600` or more restrictive.
      remediation:
        - desc: |-
            Run the below command (based on the file location on your system) on the each worker node. For example,

            ```
            chmod 600 /etc/systemd/system/kubelet.service.d/kubeadm.conf
            ```
  - uid: cis-kubernetes--5.1.1
    title: Ensure that the cluster-admin role is only used where required
    impact: 100
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.1
    props:
      - uid: cis-kubernetes--5.1.1
        title: Defines a list 'names' to be allowed in the 'subject' field of the (cluster)rolebinding `cluster-admin`.\nBy default this is only the group "system:masters"
        mql: |
          return [
          "system:masters",
          ]
      - uid: cis-kubernetes--5.1.1
        title: CIS demands only the clusterrolebindings to be checked for compliance.\nFrom a security perspective we should also check the rolebindings.\nThis property, (by default 'false'), when set to 'true' will also check the rolebindings for any subjects being allowed to use the 'cluster-admin' role.
        mql: |
          return false
      - uid: cis-kubernetes--5.1.1
        title: Defines a list of namespaces to ignore.\nFor example "kube-system".
        mql: |
          return [
            "insert-list-of-namespaces-to-ignore-here",
            #"kube-system",
          ]
    mql: |
      k8s.clusterrolebindings.where(roleRef['name'] == "cluster-admin")
        .all(
          subjects.all(
            _['name'].in(props.allowedSubjectsToHaveClusterAdminRole)
          )
        )
      k8s.rolebindings.where(roleRef['name'] == "cluster-admin")
        .where(id != empty == props.checkRolebindingsBoolean)
        .all(
          subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToHaveClusterAdminRole)
          )
        )
    docs:
      desc: |-
        The RBAC role `cluster-admin` provides wide-ranging powers over the environment and should be used only where and when needed.

        **Rationale:**

        Kubernetes provides a set of default roles where RBAC is used. Some of these roles such as `cluster-admin` provide wide-ranging privileges which should only be applied where absolutely necessary. Roles such as `cluster-admin` allow super-user access to perform any action on any resource. When used in a `ClusterRoleBinding`, it gives full control over every resource in the cluster and in all namespaces. When used in a `RoleBinding`, it gives full control over every resource in the rolebinding's namespace, including the namespace itself.

        **Impact:**

        Care should be taken before removing any `clusterrolebindings` from the environment to ensure they were not required for operation of the cluster. Specifically, modifications should not be made to `clusterrolebindings` with the `system:` prefix as they are required for the operation of system components.
      audit: |-
        Obtain a list of the principals who have access to the `cluster-admin` role by reviewing the `clusterrolebinding` output for each role binding that has access to the `cluster-admin` role.

        ```
        kubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name
        ```

        Review each principal listed and ensure that `cluster-admin` privilege is required for it.
      remediation:
        - desc: |-
            Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges.

            Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role :

            ```
            kubectl delete clusterrolebinding [name]
            ```
  - uid: cis-kubernetes--5.1.10
    title: Minimize access to the proxy sub-resource of nodes
    impact: 90
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.10
    props:
      - uid: cis-kubernetes--5.1.10
        title: Defines a list 'names' to be allowed in the 'subject' field of any (cluster)rolebinding that provides access to 'nodes/proxy'.\nBy default this is empty.\nOn GKE this is only the user "kube-apiserver".\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return [
            "kube-apiserver"
            ]
          }
          else {
            return ['']
          }
      - uid: cis-kubernetes--5.1.10
        title: Defines a list 'names' to be allowed in the 'subject' field of any (cluster)rolebinding that provides access to 'nodes/proxy'.\nBy default this empty.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return ['']
          }
          else {
            return ['']
          }
      - uid: cis-kubernetes--5.1.10
        title: Defines a list of namespaces to ignore.\nFor example "kube-system".
        mql: |
          return [
            "insert-list-of-namespaces-to-ignore-here",
            #"kube-system",
          ]
    mql: |
      // 1. all cluster roles that have access to the resource "nodes/proxy"
      dangerousNodesProxyClusterRoles = k8s.clusterroles
        .where(rules.where(_['resources'].any(_ == "nodes/proxy"))).map(name)

      // 2. check if subjects of clusterrolebindings are allowed access said cluster-role (scope cluster)
      k8s.clusterrolebindings.where(roleRef['name'].in(dangerousNodesProxyClusterRoles)).all(
        subjects.all(
            _['name'].in(props.allowedSubjectsToHaveNodeProxyAccessViaClusterrolebinding)
            )
      )

      // 3. check if subjects of rolebindings are allowed to access said cluster-role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(dangerousNodesProxyClusterRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToHaveNodeProxyAccessViaRolebinding)
            )
      )
    docs:
      desc: |-
        Users with access to the `Proxy` sub-resource of `Node` objects automatically have permissions to use the Kubelet API, which may allow for privilege escalation or bypass cluster security controls such as audit logs.

        The Kubelet provides an API which includes rights to execute commands in any container running on the node. Access to this API is covered by permissions to the main Kubernetes API via the `node` object. The proxy sub-resource specifically allows wide ranging access to the Kubelet API.

        Direct access to the Kubelet API bypasses controls like audit logging (there is no audit log of Kubelet API access) and admission control.

        **Rationale:**

        The ability to use the `proxy` sub-resource of `node` objects opens up possibilities for privilege escalation and should be restricted, where possible.
      audit: Review the users who have access to the `proxy` sub-resource of `node` objects in the Kubernetes API.
      remediation:
        - desc: Where possible, remove access to the `proxy` sub-resource of `node` objects.
  - uid: cis-kubernetes--5.1.11
    title: Minimize access to the approval sub-resource of certificatesigningrequests objects
    impact: 90
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.11
    props:
      - uid: cis-kubernetes--5.1.11
        title: Defines a list 'names' to be allowed in the 'subject' field of any clusterrolebinding that provides rights on certificatesigningrequests/approval.\nBy default this is only the Service Account "certificate-controller"\nIn GKE this is the "system:gcp-controller-manager" user.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return [
              'certificate-controller',
              'system:gcp-controller-manager'
              ]
          }
          else {
            return ['certificate-controller']
          }
      - uid: cis-kubernetes--5.1.11
        title: Defines a list 'names' to be allowed in the 'subject' field of any rolebinding that provides rights on certificatesigningrequests/approval.\nBy default this is only the Service Account "certificate-controller"\nIn GKE this is the "system:gcp-controller-manager" user.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return ['']
          }
          else {
            return ['']
          }
      - uid: cis-kubernetes--5.1.11
        title: Defines a list of namespaces to ignore.\nFor example "kube-system".
        mql: |
          return [
            "insert-list-of-namespaces-to-ignore-here",
            #"kube-system",
          ]
    mql: |
      // 1. all cluster roles that have access to the resource "certificatesigningrequests/approval" with the verbs '*' or 'update'
      dangerousCertificateRequestSiginingClusterRoles = k8s.clusterroles
        .where(rules.where(_['resources'].any(_ == "certificatesigningrequests/approval")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "update")))
            .map(name)

      // 2. all roles that have access to the resource "certificatesigningrequests/approval" with the verbs '*' or 'update'
      dangerousCertificateRequestSiginingRoles = k8s.roles
        .where(rules.where(_['resources'].any(_ == "certificatesigningrequests/approval")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "update")))
            .map(name)

        // 3. check if subjects of clusterrolebindings are allowed access said cluster-role (scope cluster)
      k8s.clusterrolebindings.where(roleRef['name'].in(dangerousCertificateRequestSiginingClusterRoles)).all(
        subjects.all(
            _['name'].in(props.allowedSubjectsToHaveCSRApprovalRightsViaClusterrolebinding)
            )
      )

        // 4. check if subjects of rolebindings are allowed access said cluster-role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(dangerousCertificateRequestSiginingClusterRoles)).all(
        subjects.where(
          _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToHaveCSRApprovalRightsViaRolebinding)
          )
      )

        // 5. check if subjects of rolebindings are allowed access said role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(dangerousCertificateRequestSiginingRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToHaveCSRApprovalRightsViaRolebinding)
            )
      )
    docs:
      desc: |-
        Users with access to the update the `approval` sub-resource of `certificateaigningrequest` objects can approve new client certificates for the Kubernetes API effectively allowing them to create new high-privileged user accounts.

        This can allow for privilege escalation to full cluster administrator, depending on users configured in the cluster

        **Rationale:**

        The ability to update certificate signing requests should be limited.
      audit: Review the users who have access to update the `approval` sub-resource of `certificatesigningrequest` objects in the Kubernetes API.
      remediation:
        - desc: Where possible, remove access to the `approval` sub-resource of `certificatesigningrequest` objects.
  - uid: cis-kubernetes--5.1.12
    title: Minimize access to webhook configuration objects
    impact: 90
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.12
    props:
      - uid: cis-kubernetes--5.1.12
        title: Defines a list 'names' to be allowed in the 'subject' field of any clusterrolebinding that provides rights on 'validatingwebhookconfigurations' or 'mutatingwebhookconfigurations' with the verbs '*' or 'create/modify/delete'.\nBy default this is empty.\nIn GKE those are the "operator" service account, the 'system:clustermetrics' user and the 'system:gke-common-webhooks' user.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return [
              'operator',
              'system:clustermetrics',
              'system:gke-common-webhooks'
              ]
          }
          else {
            return ['']
          }
      - uid: cis-kubernetes--5.1.12
        title: Defines a list 'names' to be allowed in the 'subject' field of any rolebinding that provides rights on 'validatingwebhookconfigurations' or 'mutatingwebhookconfigurations' with the verbs '*' or 'create/modify/delete'.\nBy default this is empty.\nIn GKE this is the "system:clustermetrics" user.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return ['system:clustermetrics']
          }
          else {
            return ['']
          }
      - uid: cis-kubernetes--5.1.12
        title: Defines a list of namespaces to ignore.\nFor example "kube-system".
        mql: |
          return [
            "insert-list-of-namespaces-to-ignore-here",
            #"kube-system",
          ]
    mql: |
      // 1. all cluster roles that have access to the resource "validatingwebhookconfigurations" or "mutatingwebhookconfigurations" with the verbs '*' or 'create/modify/delete'
      webhookDangerousConfClusterRoles = k8s.clusterroles
        .where(rules.where(_['resources'].any(_ == "validatingwebhookconfigurations" || _ == "mutatingwebhookconfigurations")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "create" || _ == "modify" || _ == "delete")))
            .map(name)

      // 2. all roles that have access to the resource "validatingwebhookconfigurations" or "mutatingwebhookconfigurations" with the verbs '*' or 'create/modify/delete'
      webhookDangerousConfRoles = k8s.clusterroles
        .where(rules.where(_['resources'].any(_ == "validatingwebhookconfigurations" || _ == "mutatingwebhookconfigurations")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "create" || _ == "modify" || _ == "delete")))
            .map(name)

      // 3. check if subjects of clusterrolebindings are allowed access said cluster-role (scope cluster)
      k8s.clusterrolebindings.where(roleRef['name'].in(webhookDangerousConfClusterRoles)).all(
        subjects.all(
            _['name'].in(props.allowedSubjectsToHaveWebHookRightsViaClusterrolebinding)
            )
      )

      // 4. check if subjects of rolebindings are allowed access said cluster-role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(webhookDangerousConfClusterRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToHaveWebHookRightsViaRolebinding)
            )
      )

      // 5. check if subjects of rolebindings are allowed access said role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(webhookDangerousConfRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToHaveWebHookRightsViaRolebinding)
          )
      )
    docs:
      desc: |-
        Users with rights to create/modify/delete `validatingwebhookconfigurations` or `mutatingwebhookconfigurations` can control webhooks that can read any object admitted to the cluster, and in the case of mutating webhooks, also mutate admitted objects. This could allow for privilege escalation or disruption of the operation of the cluster.

        **Rationale:**

        The ability to manage webhook configuration should be limited
      audit: Review the users who have access to `validatingwebhookconfigurations` or `mutatingwebhookconfigurations` objects in the Kubernetes API.
      remediation:
        - desc: Where possible, remove access to the `validatingwebhookconfigurations` or `mutatingwebhookconfigurations` objects
  - uid: cis-kubernetes--5.1.13
    title: Minimize access to the service account token creation
    impact: 90
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.13
    props:
      - uid: cis-kubernetes--5.1.13
        title: Defines a list 'names' to be allowed in the 'subject' field of any clusterrolebinding that have access to create the `token` sub-resource of `serviceaccount` objects in the Kubernetes API.\nBy default this is the user "system:kube-controller-manager".\nIn GKE those are the user "system:kube-controller-manager" and the user 'system:cloud-controller-manager'.\n\nRemove or add subject names as necessary.
        mql: |
          return ['system:kube-controller-manager']
      - uid: cis-kubernetes--5.1.13
        title: Defines a list 'names' to be allowed in the 'subject' field of any rolebinding that have access to create the `token` sub-resource of `serviceaccount` objects in the Kubernetes API.\nBy default this is empty.\n\nRemove or add subject names as necessary.
        mql: |
          return ['']
      - uid: cis-kubernetes--5.1.13
        title: Defines a list of namespaces to ignore.\nFor example "kube-system".
        mql: |
          return [
            "insert-list-of-namespaces-to-ignore-here",
            #"kube-system",
          ]
    mql: |
      // 1. all cluster roles that have access to the resource "serviceaccounts/token" with the verbs '*' or 'create'
      dangerousSATokenClusterRoles = k8s.clusterroles
        .where(rules.where(_['resources'].any(_ == "serviceaccounts/token")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "create")))
            .map(name)

      // 2. all cluster roles that have access to the resource "serviceaccounts/token" with the verbs '*' or 'create'
      dangerousSATokenRoles = k8s.roles
        .where(rules.where(_['resources'].any(_ == "serviceaccounts/token")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "create")))
            .map(name)

      // 3. check if subjects of clusterrolebindings are allowed access said cluster-role (scope cluster)
      k8s.clusterrolebindings.where(roleRef['name'].in(dangerousSATokenClusterRoles)).all(
        subjects.all(
            _['name'].in(props.allowedSubjectsToHaveSATokenRightsViaClusterrolebinding)
            )
      )

      // 4. check if subjects of rolebindings are allowed access said cluster-role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(dangerousSATokenClusterRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToHaveSATokenRightsViaRolebinding)
            )
      )

      // 5. check if subjects of rolebindings are allowed access said role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(dangerousSATokenRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToHaveSATokenRightsViaRolebinding)
            )
      )
    docs:
      desc: |-
        Users with rights to create new service account tokens at a cluster level, can create long-lived privileged credentials in the cluster. This could allow for privilege escalation and persistent access to the cluster, even if the users account has been revoked.

        **Rationale:**

        The ability to create service account tokens should be limited.
      audit: Review the users who have access to create the `token` sub-resource of `serviceaccount` objects in the Kubernetes API.
      remediation:
        - desc: Where possible, remove access to the `token` sub-resource of `serviceaccount` objects.
  - uid: cis-kubernetes--5.1.2
    title: Minimize access to secrets
    impact: 100
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.2
    props:
      - uid: cis-kubernetes--5.1.2
        title: Defines a list 'names' to be allowed in the 'subject' field of any clusterrolebinding that  have `get`, `list` or `watch` access to `secrets` objects in the Kubernetes API.\nA curated list of defaults have been already added below.\n\nRemove or add subject names as necessary.
        mql: |
          return [
            "system:masters",
            #"default",
            #"expand-controller",
            #"generic-garbage-collector",
            #"horizontal-pod-autoscaler",
            #"namespace-controller",
            #"persistent-volume-binder",
            #"resourcequota-controller",
            "system:kube-controller-manager"
          ]
      - uid: cis-kubernetes--5.1.2
        title: Defines a list 'names' to be allowed in the 'subject' field of any rolebinding that  have `get`, `list` or `watch` access to `secrets` objects in the Kubernetes API.\nThe defaults are empty.\n\nRemove or add subject names as necessary.
        mql: |
          return ['']
      - uid: cis-kubernetes--5.1.2
        title: Defines a list of namespaces to ignore.\nFor example "kube-system".
        mql: |
          return [
            "insert-list-of-namespaces-to-ignore-here",
            #"kube-system",
          ]
    mql: |
      // 1. all cluster roles that have access to the resource "secrets" with the verbs '*' or 'create'
      dangerousSecretsClusterRoles = k8s.clusterroles
        .where(rules.where(_['resources'].any(_ == "secrets" || _ == "*")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "get" || _ == "watch" || _ == "list")))
            .map(name)

      // 2. all roles that have access to the resource "secrets" with the verbs '*' or 'create'
      dangerousSecretsRoles = k8s.roles
        .where(rules.where(_['resources'].any(_ == "secrets" || _ == "*")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "get" || _ == "watch" || _ == "list")))
            .map(name)

      // 3. check if subjects of clusterrolebindings are allowed access said cluster-role (scope cluster)
      k8s.clusterrolebindings.where(roleRef['name'].in(dangerousSecretsClusterRoles)).all(
        subjects.all(
            _['name'].in(props.allowedSubjectsToGetSecretsViaClusterrolebinding)
          )
      )

      // 4. check if subjects of rolebindings are allowed access said cluster-role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(dangerousSecretsClusterRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToGetSecretsViaRolebinding)
            )
      )

      // 5. check if subjects of rolebindings are allowed access said cluster-role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(dangerousSecretsRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToGetSecretsViaRolebinding)
            )
      )
    docs:
      desc: |-
        The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.

        **Rationale:**

        Inappropriate access to secrets stored within the Kubernetes cluster can allow for an attacker to gain additional access to the Kubernetes cluster or external resources whose credentials are stored as secrets.

        **Impact:**

        Care should be taken not to remove access to secrets to system components which require this for their operation
      audit: Review the users who have `get`, `list` or `watch` access to `secrets` objects in the Kubernetes API.
      remediation:
        - desc: Where possible, remove `get`, `list` and `watch` access to `secret` objects in the cluster.
  - uid: cis-kubernetes--5.1.4
    title: Minimize access to create pods
    impact: 95
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.4
    props:
      - uid: cis-kubernetes--5.1.4
        title: Defines a list 'names' to be allowed in the 'subject' field of any clusterrolebinding that  have `create` access to `pod` objects in the Kubernetes API.\nThe defaults have been already added below.\n\nRemove or add subject names as necessary.
        mql: |
          return [
            "system:masters",
            "default",
            "attachdetach-controller",
            "cronjob-controller",
            "daemon-set-controller",
            "deployment-controller",
            "endpoint-controller",
            "endpointslice-controller",
            "ephemeral-volume-controller",
            "generic-garbage-collector",
            "horizontal-pod-autoscaler",
            "job-controller",
            "node-controller",
            "persistent-volume-binder",
            "pvc-protection-controller",
            "replicaset-controller",
            "replication-controller",
            "resourcequota-controller",
            "statefulset-controller",
            "system:kube-controller-manager",
            "system:kube-scheduler"
          ]
      - uid: cis-kubernetes--5.1.4
        title: Defines a list 'names' to be allowed in the 'subject' field of any rolebinding that  have `create` access to `pod` objects in the Kubernetes API.\nThe defaults have been already added below.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return [
              'operator'
              ]
          }
          else {
            return ['']
          }
      - uid: cis-kubernetes--5.1.4
        title: Defines a list of namespaces to ignore.\nFor example "kube-system".
        mql: |
          return [
            "insert-list-of-namespaces-to-ignore-here",
            #"kube-system",
          ]
    mql: |
      // 1. all cluster roles that have access to the resource "pods" with the verbs '*' or 'create'
      dangerousPodClusterRoles = k8s.clusterroles
        .where(rules.where(_['resources'].any(_ == "pods" || _ == "*")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "create")))
            .map(name)

      // 2. all roles that have access to the resource "pods" with the verbs '*' or 'create'
      dangerousPodRoles = k8s.roles
        .where(rules.where(_['resources'].any(_ == "pods" || _ == "*")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "create")))
            .map(name)

      // 3. check if subjects of clusterrolebindings are allowed access said cluster-role (scope cluster)
      k8s.clusterrolebindings.where(roleRef['name'].in(dangerousPodClusterRoles)).all(
        subjects.all(
            _['name'].in(props.allowedSubjectsToCreatePodsViaClusterrolebinding)
            )
      )

        // 4. check if subjects of clusterrolebindings are allowed access said role (scope namespace)
      k8s.clusterrolebindings.where(roleRef['name'].in(dangerousPodRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToCreatePodsViaClusterrolebinding)
            )
       )

        // 5. check if subjects of rolebindings are allowed access said roles (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(dangerousPodRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToCreatePodsViaRolebinding)
            )
      )
    docs:
      desc: |-
        The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access)

        As such, access to create new pods should be restricted to the smallest possible group of users.

        **Rationale:**

        The ability to create pods in a cluster opens up possibilities for privilege escalation and should be restricted, where possible.

        **Impact:**

        Care should be taken not to remove access to pods to system components which require this for their operation
      audit: Review the users who have create access to pod objects in the Kubernetes API.
      remediation:
        - desc: Where possible, remove `create` access to `pod` objects in the cluster.
  - uid: cis-kubernetes--5.1.5
    title: Ensure that default service accounts are not actively used.
    impact: 60
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.5
    props:
      - uid: cis-kubernetes--5.1.5
        title: Defines a list of namespaces that should not be checked
        mql: |
          if (asset.version == /eks/) {
            return ["kube-system", "kube-node-lease", "kube-public"]
          }
          if (asset.version == /gke/) {
            return ["kube-system", "kube-node-lease", "kube-public","gmp-public","gmp-system","gke-managed-cim","gke-managed-system"]
          }
          return ["kube-system", "gatekeeper-system", "kube-node-lease", "kube-public","tigera-operator","calico-system"]
    mql: |
      k8s.serviceaccounts.where(namespace.contains(props.excludedNamespaces) != true)
        .where(name == "default")
          .all(automountServiceAccountToken == false)

      k8s.rolebindings.where(subjects.where(kind == "ServiceAccount"))
        .where(namespace.contains(props.excludedNamespaces) != true)
          .all(subjects.all(name != "default"))

      k8s.clusterrolebindings.where(subjects.where(kind == "ServiceAccount"))
        .where(subjects.where(_['namespace'].in(props.excludedNamespaces) != true))
          .all(subjects.all(name != "default"))
    docs:
      desc: |-
        The `default` service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.

        **Rationale:**

        Kubernetes provides a `default` service account which is used by cluster workloads where no specific service account is assigned to the pod.

        Where access to the Kubernetes API from a pod is required, a specific service account should be created for that pod, and rights granted to that service account.

        The default service account should be configured such that it does not provide a service account token and does not have any explicit rights assignments.

        **Impact:**

        All workloads which require access to the Kubernetes API will require an explicit service account to be created.
      audit: |-
        For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults.

        Additionally ensure that the `automountServiceAccountToken: false` setting is in place for each default service account.
      remediation:
        - desc: |-
            Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.

            Modify the configuration of each default service account to include this value

            ```
            automountServiceAccountToken: false
            ```
  - uid: cis-kubernetes--5.1.6
    title: Ensure that Service Account Tokens are only mounted where necessary
    impact: 60
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.6
    props:
      - uid: cis-kubernetes--5.1.6
        title: Defines a list of namespaces that should not be checked
        mql: |
          if (asset.version == /eks/) {
            return ["kube-system", "kube-node-lease", "kube-public"]
          }
          if (asset.version == /gke/) {
            return ["kube-system", "kube-node-lease", "kube-public","gmp-public","gmp-system","gke-managed-cim","gke-managed-system"]
          }
          return ["kube-system", "gatekeeper-system", "kube-node-lease", "kube-public","tigera-operator","calico-system"]
    mql: |
      k8s.serviceaccounts.where(namespace.contains(props.excludedNamespaces) != true)
        .where(name == "default")
          .all(automountServiceAccountToken == false)
    docs:
      desc: |-
        Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server

        **Rationale:**

        Mounting service account tokens inside pods can provide an avenue for privilege escalation attacks where an attacker is able to compromise a single pod in the cluster.

        Avoiding mounting these tokens removes this attack avenue.

        **Impact:**

        Pods mounted without service account tokens will not be able to communicate with the API server, except where the resource is available to unauthenticated principals.
      audit: |-
        Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access.

        ```
        automountServiceAccountToken: false
        ```
      remediation:
        - desc: Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.
  - uid: cis-kubernetes--5.1.8
    title: Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster
    impact: 90
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.8
    props:
      - uid: cis-kubernetes--5.1.8
        title: Defines a list 'names' to be allowed in the 'subject' field of any clusterrolebinding that provides rights to 'bind/escalate/impersonate'.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return [
              "system:masters",
              "kube-apiserver",
              "clusterrole-aggregation-controller",
              "system:gcp-controller-manager",
              "system:gke-common-webhooks",
              "system:gke-master-resourcequota",
              "system:controller:glbc",
              "system:l7-lb-controller",
              "system:kubestore-collector",
              "system:managed-certificate-controller",
              "system:metrics-server-nanny"
              ]
          }
          else {
            return [
              "system:masters",
              "clusterrole-aggregation-controller"
              ]
          }
      - uid: cis-kubernetes--5.1.8
        title: Defines a list 'names' to be allowed in the 'subject' field of any rolebinding that provides rights to 'bind/escalate/impersonate'.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return ['']
          }
          else {
            return ['']
          }
      - uid: cis-kubernetes--5.1.8
        title: Defines a list of namespaces to ignore.\nFor example "kube-system".
        mql: |
          return [
            "insert-list-of-namespaces-to-ignore-here",
            #"kube-system",
          ]
    mql: |
      // 1. all cluster roles that have access to the the verbs '*' or 'bind/impersonate/escalate'
      escalateClusterRoles = k8s.clusterroles.where(rules.where(_['verbs'].any(_ == "*" || _ == "bind" || _ == "impersonate" || _ == "escalate")))
            .map(name)

      // 2. all cluster roles that have access to the the verbs '*' or 'bind/impersonate/escalate'
      escalateRoles = k8s.roles.where(rules.where(_['verbs'].any(_ == "*" || _ == "bind" || _ == "impersonate" || _ == "escalate")))
            .map(name)

        // 3. check if subjects of clusterrolebindings are allowed access said cluster-role (scope cluster)
      k8s.clusterrolebindings.where(roleRef['name'].in(escalateClusterRoles)).all(
        subjects.all(
            _['name'].in(props.allowedSubjectsToEscalateViaClusterrolebinding)
            )
      )

        // 4. check if subjects of rolebindings are allowed access said cluster-role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(escalateClusterRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToEscalateViaRolebinding)
            )
      )

        // 5. check if subjects of rolebindings are allowed access said role (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(escalateRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToEscalateViaRolebinding)
            )
      )
    docs:
      desc: |-
        Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators

        **Rationale:**

        The impersonate privilege allows a subject to impersonate other users gaining their rights to the cluster. The bind privilege allows the subject to add a binding to a cluster role or role which escalates their effective permissions in the cluster. The escalate privilege allows a subject to modify cluster roles to which they are bound, increasing their rights to that level.

        Each of these permissions has the potential to allow for privilege escalation to cluster-admin level.

        **Impact:**

        There are some cases where these permissions are required for cluster service operation, and care should be taken before removing these permissions from system service accounts.
      audit: Review the users who have access to cluster roles or roles which provide the impersonate, bind or escalate privileges.
      remediation:
        - desc: Where possible, remove the impersonate, bind and escalate rights from subjects.
  - uid: cis-kubernetes--5.1.9
    title: Minimize access to create persistent volumes
    impact: 95
    filters: asset.platform == "k8s-cluster"
    tags:
      cisecurity.org/recommendation: 5.1.9
    props:
      - uid: cis-kubernetes--5.1.9
        title: Defines a list 'names' to be allowed in the 'subject' field of any clusterrolebinding that  have `create` access to `pod` objects in the Kubernetes API.\nThe defaults have been already added below.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return [
              "system:masters",
              "system:cloud-controller-manager",
              "generic-garbage-collector",
              "horizontal-pod-autoscaler",
              "resourcequota-controller",
              "system:gke-common-webhooks",
              "system:vpa-recommender",
              "system:kube-controller-manager",
              "system:kubestore-collector"
            ]
          }
          else {
            return [
              "system:masters",
              "default",
              "generic-garbage-collector",
              "horizontal-pod-autoscaler",
              "resourcequota-controller",
              "system:kube-controller-manager"
            ]
          }
      - uid: cis-kubernetes--5.1.9
        title: Defines a list 'names' to be allowed in the 'subject' field of any rolebinding that  have `create` access to `pod` objects in the Kubernetes API.\nThe defaults have been already added below.\n\nRemove or add subject names as necessary.
        mql: |
          if (asset.version == /-gke./) {
            return [
              "operator",
              ]
          }
          else {
            return ['']
          }
      - uid: cis-kubernetes--5.1.9
        title: Defines a list of namespaces to ignore.\nFor example "kube-system".
        mql: |
          return [
            "insert-list-of-namespaces-to-ignore-here",
            #"kube-system",
          ]
    mql: |
      // 1. all cluster roles that have access to the resource "PersistentVolume" with the verbs '*' or 'create'
      createPersistentVolumeClusterRoles = k8s.clusterroles
        .where(rules.where(_['resources'].any(_ == "PersistentVolume" || _ == "*")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "create")))
            .map(name)

      // 2. all roles that have access to the resource "PersistentVolume" with the verbs '*' or 'create'
      createPersistentVolumeRoles  = k8s.roles
        .where(rules.where(_['resources'].any(_ == "pods" || _ == "*")))
          .where(rules.where(_['verbs'].any(_ == "*" || _ == "create")))
            .map(name)

      // 3. check if subjects of clusterrolebindings are allowed access said cluster-role (scope cluster)
      k8s.clusterrolebindings.where(roleRef['name'].in(createPersistentVolumeClusterRoles)).all(
        subjects.all(
            _['name'].in(props.allowedSubjectsToCreatePodsViaClusterrolebinding)
            )
      )

        // 4. check if subjects of clusterrolebindings are allowed access said role (scope namespace)
      k8s.clusterrolebindings.where(roleRef['name'].in(createPersistentVolumeRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToCreatePodsViaClusterrolebinding)
            )
      )

        // 5. check if subjects of rolebindings are allowed access said roles (scope namespace)
      k8s.rolebindings.where(roleRef['name'].in(createPersistentVolumeRoles)).all(
        subjects.where(
            _['namespace'].in(props.ignoredNamespaces) != true
          ).all(
            _['name'].in(props.allowedSubjectsToCreatePodsViaRolebinding)
            )
      )
    docs:
      desc: |-
        The ability to create persistent volumes in a cluster can provide an opportunity for privilege escalation, via the creation of `hostPath` volumes. As persistent volumes are not covered by Pod Security Admission, a user with access to create persistent volumes may be able to get access to sensitive files from the underlying host even where restrictive Pod Security Admission policies are in place.

        **Rationale:**

        The ability to create persistent volumes in a cluster opens up possibilities for privilege escalation and should be restricted, where possible.
      audit: Review the users who have create access to `PersistentVolume` objects in the Kubernetes API.
      remediation:
        - desc: Where possible, remove `create` access to `PersistentVolume` objects in the cluster.
  - uid: cis-kubernetes--5.2.1
    title: Ensure that the cluster has at least one active policy control mechanism in place
    impact: 95
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.1
    props:
      - uid: cis-kubernetes--5.2.1
        title: Defines a list namespaces which are exempt from testing if the default Pod Security Admission controller is enabled
        mql: |
          return [
          'list-of-exempt-namespaces-names-here'
          ]
    variants:
      - uid: k8s-ensure-cluster-has-at-least-one-policy-control-mechanism-cluster
      - uid: k8s-ensure-cluster-has-at-least-one-policy-control-mechanism-namespace
    docs:
      desc: |-
        Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.

        **Rationale:**

        Without an active policy control mechanism, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts.

        **Impact:**

        Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.
      audit: Review the workloads deployed to the cluster to understand if Pod Security Admission or external admission control systems are in place.
      remediation:
        - desc: Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads.
  - uid: cis-kubernetes--5.2.11
    title: Minimize the admission of Windows HostProcess Containers
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.11
    props:
      - uid: cis-kubernetes--5.2.11
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-windows-hostprocess-containers-cluster
      - uid: k8s-minimize-the-admission-of-windows-hostprocess-containers-namespace
    docs:
      desc: |-
        Do not generally permit Windows containers to be run with the `hostProcess` flag set to true.

        **Rationale:**

        A Windows container making use of the `hostProcess` flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides "privileged access" to the Windows node.

        Where Windows containers are used inside a Kubernetes cluster, there should be at least one admission control policy which does not permit `hostProcess` Windows containers.

        If you need to run Windows containers which require `hostProcess`, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `securityContext.windowsOptions.hostProcess: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostProcess` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostProcess` containers.
  - uid: cis-kubernetes--5.2.12
    title: Minimize the admission of HostPath volumes
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.12
    props:
      - uid: cis-kubernetes--5.2.12
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-hostpath-volumes-cluster
      - uid: k8s-minimize-the-admission-of-hostpath-volumes-namespace
    docs:
      desc: |-
        Do not generally admit containers which make use of `hostPath` volumes.

        **Rationale:**

        A container which mounts a `hostPath` volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of `hostPath` volumes may allow containers access to privileged areas of the node filesystem.

        There should be at least one admission control policy defined which does not permit containers to mount `hostPath` volumes.

        If you need to run containers which require `hostPath` volumes, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined which make use of `hostPath` volumes will not be permitted unless they are run under a spefific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with `hostPath` volumes.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use `hostPath` volumes.
  - uid: cis-kubernetes--5.2.13
    title: Minimize the admission of containers which use HostPorts
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.13
    props:
      - uid: cis-kubernetes--5.2.13
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-containers-which-use-hostports-cluster
      - uid: k8s-minimize-the-admission-of-containers-which-use-hostports-namespace
    docs:
      desc: |-
        Do not generally permit containers which require the use of HostPorts.

        **Rationale:**

        Host ports connect containers directly to the host's network. This can bypass controls such as network policy.

        There should be at least one admission control policy defined which does not permit containers which require the use of HostPorts.

        If you need to run containers which require HostPorts, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `hostPort` settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have `hostPort` sections.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use `hostPort` sections.
  - uid: cis-kubernetes--5.2.2
    title: Minimize the admission of privileged containers
    impact: 90
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.2
    props:
      - uid: cis-kubernetes--5.2.2
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-privileged-containers-cluster
      - uid: k8s-minimize-the-admission-of-privileged-containers-namespace
    docs:
      desc: |-
        Do not generally permit containers to be run with the `securityContext.privileged` flag set to `true`.

        **Rationale:**

        Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices.

        There should be at least one admission control policy defined which does not permit privileged containers.

        If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.containers[].securityContext.privileged: true`, `spec.initContainers[].securityContext.privileged: true` and `spec.ephemeralContainers[].securityContext.privileged: true` will not be permitted.
      audit: |-
        Run the following command:

        ```
        get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}'
        ```

        It will produce an inventory of all the privileged use on the cluster, if any (please, refer to a sample below). Further grepping can be done to automate each specific violation detection.

        calico-kube-controllers-57b57c56f-jtmk4: {} << No Elevated Privileges calico-node-c4xv4: {} {"privileged":true} {"privileged":true} {"privileged":true} {"privileged":true} << Violates 5.2.2 dashboard-metrics-scraper-7bc864c59-2m2xw: {"seccompProfile":{"type":"RuntimeDefault"}} {"allowPrivilegeEscalation":false,"readOnlyRootFilesystem":true,"runAsGroup":2001,"runAsUser":1001}
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers.
  - uid: cis-kubernetes--5.2.3
    title: Minimize the admission of containers wishing to share the host process ID namespace
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.3
    props:
      - uid: cis-kubernetes--5.2.3
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-containers-share-host-pid-cluster
      - uid: k8s-minimize-the-admission-of-containers-share-host-pid-namespace
    docs:
      desc: |-
        Do not generally permit containers to be run with the `hostPID` flag set to true.

        **Rationale:**

        A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container.

        There should be at least one admission control policy defined which does not permit containers to share the host PID namespace.

        If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.hostPID: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostPID` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostPID` containers.
  - uid: cis-kubernetes--5.2.4
    title: Minimize the admission of containers wishing to share the host IPC namespace
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.4
    props:
      - uid: cis-kubernetes--5.2.4
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-containers-share-host-ipc-cluster
      - uid: k8s-minimize-the-admission-of-containers-share-host-ipc-namespace
    docs:
      desc: |-
        Do not generally permit containers to be run with the `hostIPC` flag set to true.

        **Rationale:**

        A container running in the host's IPC namespace can use IPC to interact with processes outside the container.

        There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace.

        If you need to run containers which require hostIPC, this should be definited in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.hostIPC: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostIPC` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostIPC` containers.
  - uid: cis-kubernetes--5.2.5
    title: Minimize the admission of containers wishing to share the host network namespace
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.5
    props:
      - uid: cis-kubernetes--5.2.5
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-containers-host-network-cluster
      - uid: k8s-minimize-the-admission-of-containers-host-network-namespace
    docs:
      desc: |-
        Do not generally permit containers to be run with the `hostNetwork` flag set to true.

        **Rationale:**

        A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods.

        There should be at least one admission control policy defined which does not permit containers to share the host network namespace.

        If you need to run containers which require access to the host's network namesapces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.hostNetwork: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostNetwork` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostNetwork` containers.
  - uid: cis-kubernetes--5.2.6
    title: Minimize the admission of containers with allowPrivilegeEscalation
    impact: 90
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.6
    props:
      - uid: cis-kubernetes--5.2.6
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-containers-with-allowpr-cluster
      - uid: k8s-minimize-the-admission-of-containers-with-allowpr-namespace
    docs:
      desc: |-
        Do not generally permit containers to be run with the `allowPrivilegeEscalation` flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with.

        It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.

        **Rationale:**

        A container running with the `allowPrivilegeEscalation` flag set to `true` may have processes that can gain more privileges than their parent.

        There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run.

        If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.allowPrivilegeEscalation: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of conatiners with `.spec.allowPrivilegeEscalation`set to `true`.
  - uid: cis-kubernetes--5.2.8
    title: Minimize the admission of containers with the NET_RAW capability
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.8
    props:
      - uid: cis-kubernetes--5.2.8
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-containers-with-net-raw-cluster
      - uid: k8s-minimize-the-admission-of-containers-with-net-raw-namespace
    docs:
      desc: |-
        Do not generally permit containers with the potentially dangerous NET_RAW capability.

        **Rationale:**

        Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers.

        Ideally, all containers should drop this capability.

        There should be at least one admission control policy defined which does not permit containers with the NET_RAW capability.

        If you need to run containers with this capability, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods with containers which run with the NET_RAW capability will not be permitted.
      audit: List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the `NET_RAW` capability.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the `NET_RAW` capability.
  - uid: cis-kubernetes--5.2.9
    title: Minimize the admission of containers with added capabilities
    impact: 70
    filters: null
    tags:
      cisecurity.org/recommendation: 5.2.9
    props:
      - uid: cis-kubernetes--5.2.9
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    variants:
      - uid: k8s-minimize-the-admission-of-containers-with-added-capa-cluster
      - uid: k8s-minimize-the-admission-of-containers-with-added-capa-namespace
    docs:
      desc: |-
        Do not generally permit containers with capabilities assigned beyond the default set.

        **Rationale:**

        Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks.

        There should be at least one policy defined which prevents containers with capabilities beyond the default set from launching.

        If you need to run containers with additional capabilities, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods with containers which require capabilities outwith the default set will not be permitted.
      audit: List the policies in use for each namespace in the cluster, ensure that policies are present which prevent `allowedCapabilities` to be set to anything other than an empty array.
      remediation:
        - desc: Ensure that `allowedCapabilities` is not present in policies for the cluster unless it is set to an empty array.
  - uid: k8s-ensure-cluster-has-at-least-one-policy-control-mechanism-cluster
    title: Ensure that the cluster has at least one active policy control mechanism in place
    impact: 95
    filters: asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes--5.2.1
        title: Defines a list namespaces which are exempt from testing if the default Pod Security Admission controller is enabled
        mql: |
          return [
          'list-of-exempt-namespaces-names-here'
          ]
    mql: |
      k8s.namespaces.where(
        name.in(props.excludedNamespacesPolicyControl) != true
      ).all(
        labels.keys.where(
          _ == /pod-security.kubernetes.io/
            && _ ==  /audit|warn|enforce/
        )
      )
    docs:
      desc: |-
        Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.

        **Rationale:**

        Without an active policy control mechanism, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts.

        **Impact:**

        Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.
      audit: Review the workloads deployed to the cluster to understand if Pod Security Admission or external admission control systems are in place.
      remediation:
        - desc: Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads.
  - uid: k8s-ensure-cluster-has-at-least-one-policy-control-mechanism-namespace
    title: Ensure that the cluster has at least one active policy control mechanism in place
    impact: 95
    filters: asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes--5.2.1
        title: Defines a list namespaces which are exempt from testing if the default Pod Security Admission controller is enabled
        mql: |
          return [
          'list-of-exempt-namespaces-names-here'
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespacesPolicyControl.containsNone([_])) {
        k8s.namespace.labels.keys.any(_ == /pod-security.kubernetes.io/ && _ ==  /audit|warn|enforce/ )
      }
    docs:
      desc: |-
        Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.

        **Rationale:**

        Without an active policy control mechanism, it is not possible to limit the use of containers with access to underlying cluster nodes, via mechanisms like privileged containers, or the use of hostPath volume mounts.

        **Impact:**

        Where policy control systems are in place, there is a risk that workloads required for the operation of the cluster may be stopped from running. Care is required when implementing admission control policies to ensure that this does not occur.
      audit: Review the workloads deployed to the cluster to understand if Pod Security Admission or external admission control systems are in place.
      remediation:
        - desc: Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads.
  - uid: k8s-minimize-the-admission-of-containers-host-network-cluster
    title: Minimize the admission of containers wishing to share the host network namespace
    impact: 70
    filters: |
      asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes-v1-23--5_2_5-minimize-the-admission-of-containers-wishing-to-share-the-host-network-
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces4) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      )
    docs:
      desc: |-
        Do not generally permit containers to be run with the `hostNetwork` flag set to true.

        **Rationale:**

        A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods.

        There should be at least one admission control policy defined which does not permit containers to share the host network namespace.

        If you need to run containers which require access to the host's network namesapces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.hostNetwork: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostNetwork` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostNetwork` containers.
  - uid: k8s-minimize-the-admission-of-containers-host-network-namespace
    title: Minimize the admission of containers wishing to share the host network namespace
    impact: 70
    filters: |
      asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes--5.2.5
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces4.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
          k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      }
    docs:
      desc: |-
        Do not generally permit containers to be run with the `hostNetwork` flag set to true.

        **Rationale:**

        A container running in the host's network namespace could access the local loopback device, and could access network traffic to and from other pods.

        There should be at least one admission control policy defined which does not permit containers to share the host network namespace.

        If you need to run containers which require access to the host's network namesapces, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.hostNetwork: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostNetwork` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostNetwork` containers.
  - uid: k8s-minimize-the-admission-of-containers-share-host-ipc-cluster
    title: Minimize the admission of containers wishing to share the host IPC namespace
    impact: 70
    filters: |
      asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes--5.2.4
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces3) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      )
    docs:
      desc: |-
        Do not generally permit containers to be run with the `hostIPC` flag set to true.

        **Rationale:**

        A container running in the host's IPC namespace can use IPC to interact with processes outside the container.

        There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace.

        If you need to run containers which require hostIPC, this should be definited in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.hostIPC: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostIPC` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostIPC` containers.
  - uid: k8s-minimize-the-admission-of-containers-share-host-ipc-namespace
    title: Minimize the admission of containers wishing to share the host IPC namespace
    impact: 70
    filters: |
      asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes--5.2.4
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces3.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
          k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      }
    docs:
      desc: |-
        Do not generally permit containers to be run with the `hostIPC` flag set to true.

        **Rationale:**

        A container running in the host's IPC namespace can use IPC to interact with processes outside the container.

        There should be at least one admission control policy defined which does not permit containers to share the host IPC namespace.

        If you need to run containers which require hostIPC, this should be definited in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.hostIPC: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostIPC` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostIPC` containers.
  - uid: k8s-minimize-the-admission-of-containers-share-host-pid-cluster
    title: Minimize the admission of containers wishing to share the host process ID namespace
    impact: 70
    filters: |
      asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes--5.2.3
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces2) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      )
    docs:
      desc: |-
        Do not generally permit containers to be run with the `hostPID` flag set to true.

        **Rationale:**

        A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container.

        There should be at least one admission control policy defined which does not permit containers to share the host PID namespace.

        If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.hostPID: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostPID` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostPID` containers.
  - uid: k8s-minimize-the-admission-of-containers-share-host-pid-namespace
    title: Minimize the admission of containers wishing to share the host process ID namespace
    impact: 70
    filters: |
      asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes-v1-24--5_2_3-minimize-the-admission-of-containers-wishing-to-share-the-host-process-
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces2.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
          k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      }
    docs:
      desc: |-
        Do not generally permit containers to be run with the `hostPID` flag set to true.

        **Rationale:**

        A container running in the host's PID namespace can inspect processes running outside the container. If the container also has access to ptrace capabilities this can be used to escalate privileges outside of the container.

        There should be at least one admission control policy defined which does not permit containers to share the host PID namespace.

        If you need to run containers which require hostPID, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.hostPID: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostPID` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostPID` containers.
  - uid: k8s-minimize-the-admission-of-containers-which-use-hostports-cluster
    title: Minimize the admission of containers which use HostPorts
    impact: 70
    filters: asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes--5.2.13
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces12) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      )
    docs:
      desc: |-
        Do not generally permit containers which require the use of HostPorts.

        **Rationale:**

        Host ports connect containers directly to the host's network. This can bypass controls such as network policy.

        There should be at least one admission control policy defined which does not permit containers which require the use of HostPorts.

        If you need to run containers which require HostPorts, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `hostPort` settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have `hostPort` sections.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use `hostPort` sections.
  - uid: k8s-minimize-the-admission-of-containers-which-use-hostports-namespace
    title: Minimize the admission of containers which use HostPorts
    impact: 70
    filters: asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes-v1-24--5.2.13
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces12.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
          k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      }
    docs:
      desc: |-
        Do not generally permit containers which require the use of HostPorts.

        **Rationale:**

        Host ports connect containers directly to the host's network. This can bypass controls such as network policy.

        There should be at least one admission control policy defined which does not permit containers which require the use of HostPorts.

        If you need to run containers which require HostPorts, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `hostPort` settings in either the container, initContainer or ephemeralContainer sections will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have `hostPort` sections.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use `hostPort` sections.
  - uid: k8s-minimize-the-admission-of-containers-with-added-capa-cluster
    title: Minimize the admission of containers with added capabilities
    impact: 70
    filters: |
      asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes-v1-23--5_2_9-minimize-the-admission-of-containers-with-added-capabilities
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces7) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted"
      )
    docs:
      desc: |-
        Do not generally permit containers with capabilities assigned beyond the default set.

        **Rationale:**

        Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks.

        There should be at least one policy defined which prevents containers with capabilities beyond the default set from launching.

        If you need to run containers with additional capabilities, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods with containers which require capabilities outwith the default set will not be permitted.
      audit: List the policies in use for each namespace in the cluster, ensure that policies are present which prevent `allowedCapabilities` to be set to anything other than an empty array.
      remediation:
        - desc: Ensure that `allowedCapabilities` is not present in policies for the cluster unless it is set to an empty array.
  - uid: k8s-minimize-the-admission-of-containers-with-added-capa-namespace
    title: Minimize the admission of containers with added capabilities
    impact: 70
    filters: |
      asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes--5.2.9
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces7.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted"
      }
    docs:
      desc: |-
        Do not generally permit containers with capabilities assigned beyond the default set.

        **Rationale:**

        Containers run with a default set of capabilities as assigned by the Container Runtime. Capabilities outside this set can be added to containers which could expose them to risks of container breakout attacks.

        There should be at least one policy defined which prevents containers with capabilities beyond the default set from launching.

        If you need to run containers with additional capabilities, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods with containers which require capabilities outwith the default set will not be permitted.
      audit: List the policies in use for each namespace in the cluster, ensure that policies are present which prevent `allowedCapabilities` to be set to anything other than an empty array.
      remediation:
        - desc: Ensure that `allowedCapabilities` is not present in policies for the cluster unless it is set to an empty array.
  - uid: k8s-minimize-the-admission-of-containers-with-allowpr-cluster
    title: Minimize the admission of containers with allowPrivilegeEscalation
    impact: 90
    filters: |
      asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes-v1-24--5_2_6-minimize-the-admission-of-containers-with-allowprivilegeescalation
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces5) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted"
      )
    docs:
      desc: |-
        Do not generally permit containers to be run with the `allowPrivilegeEscalation` flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with.

        It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.

        **Rationale:**

        A container running with the `allowPrivilegeEscalation` flag set to `true` may have processes that can gain more privileges than their parent.

        There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run.

        If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.allowPrivilegeEscalation: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of conatiners with `.spec.allowPrivilegeEscalation`set to `true`.
  - uid: k8s-minimize-the-admission-of-containers-with-allowpr-namespace
    title: Minimize the admission of containers with allowPrivilegeEscalation
    impact: 90
    filters: |
      asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes-v1-23--5_2_6-minimize-the-admission-of-containers-with-allowprivilegeescalation
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces5.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted"
      }
    docs:
      desc: |-
        Do not generally permit containers to be run with the `allowPrivilegeEscalation` flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with.

        It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.

        **Rationale:**

        A container running with the `allowPrivilegeEscalation` flag set to `true` may have processes that can gain more privileges than their parent.

        There should be at least one admission control policy defined which does not permit containers to allow privilege escalation. The option exists (and is defaulted to true) to permit setuid binaries to run.

        If you have need to run containers which use setuid binaries or require privilege escalation, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.allowPrivilegeEscalation: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of conatiners with `.spec.allowPrivilegeEscalation`set to `true`.
  - uid: k8s-minimize-the-admission-of-containers-with-net-raw-cluster
    title: Minimize the admission of containers with the NET_RAW capability
    impact: 70
    filters: |
      asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes-v1-24--5_2_8-minimize-the-admission-of-containers-with-the-net-raw-capability
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces9) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted"
      )
    docs:
      desc: |-
        Do not generally permit containers with the potentially dangerous NET_RAW capability.

        **Rationale:**

        Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers.

        Ideally, all containers should drop this capability.

        There should be at least one admission control policy defined which does not permit containers with the NET_RAW capability.

        If you need to run containers with this capability, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods with containers which run with the NET_RAW capability will not be permitted.
      audit: List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the `NET_RAW` capability.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the `NET_RAW` capability.
  - uid: k8s-minimize-the-admission-of-containers-with-net-raw-namespace
    title: Minimize the admission of containers with the NET_RAW capability
    impact: 70
    filters: |
      asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes--5.2.8
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces9.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted"
      }
    docs:
      desc: |-
        Do not generally permit containers with the potentially dangerous NET_RAW capability.

        **Rationale:**

        Containers run with a default set of capabilities as assigned by the Container Runtime. By default this can include potentially dangerous capabilities. With Docker as the container runtime the NET_RAW capability is enabled which may be misused by malicious containers.

        Ideally, all containers should drop this capability.

        There should be at least one admission control policy defined which does not permit containers with the NET_RAW capability.

        If you need to run containers with this capability, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods with containers which run with the NET_RAW capability will not be permitted.
      audit: List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the `NET_RAW` capability.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the `NET_RAW` capability.
  - uid: k8s-minimize-the-admission-of-hostpath-volumes-cluster
    title: Minimize the admission of HostPath volumes
    impact: 70
    filters: asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes--5.2.12
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces11) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      )
    docs:
      desc: |-
        Do not generally admit containers which make use of `hostPath` volumes.

        **Rationale:**

        A container which mounts a `hostPath` volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of `hostPath` volumes may allow containers access to privileged areas of the node filesystem.

        There should be at least one admission control policy defined which does not permit containers to mount `hostPath` volumes.

        If you need to run containers which require `hostPath` volumes, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined which make use of `hostPath` volumes will not be permitted unless they are run under a spefific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with `hostPath` volumes.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use `hostPath` volumes.
  - uid: k8s-minimize-the-admission-of-hostpath-volumes-namespace
    title: Minimize the admission of HostPath volumes
    impact: 70
    filters: asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes--5.2.12
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces11.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
          k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      }
    docs:
      desc: |-
        Do not generally admit containers which make use of `hostPath` volumes.

        **Rationale:**

        A container which mounts a `hostPath` volume as part of its specification will have access to the filesystem of the underlying cluster node. The use of `hostPath` volumes may allow containers access to privileged areas of the node filesystem.

        There should be at least one admission control policy defined which does not permit containers to mount `hostPath` volumes.

        If you need to run containers which require `hostPath` volumes, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined which make use of `hostPath` volumes will not be permitted unless they are run under a spefific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with `hostPath` volumes.
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use `hostPath` volumes.
  - uid: k8s-minimize-the-admission-of-privileged-containers-cluster
    title: Minimize the admission of privileged containers
    impact: 90
    filters: |
      asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes--5.2.2
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces1) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      )
    docs:
      desc: |-
        Do not generally permit containers to be run with the `securityContext.privileged` flag set to `true`.

        **Rationale:**

        Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices.

        There should be at least one admission control policy defined which does not permit privileged containers.

        If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.containers[].securityContext.privileged: true`, `spec.initContainers[].securityContext.privileged: true` and `spec.ephemeralContainers[].securityContext.privileged: true` will not be permitted.
      audit: |-
        Run the following command:

        ```
        get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}'
        ```

        It will produce an inventory of all the privileged use on the cluster, if any (please, refer to a sample below). Further grepping can be done to automate each specific violation detection.

        calico-kube-controllers-57b57c56f-jtmk4: {} << No Elevated Privileges calico-node-c4xv4: {} {"privileged":true} {"privileged":true} {"privileged":true} {"privileged":true} << Violates 5.2.2 dashboard-metrics-scraper-7bc864c59-2m2xw: {"seccompProfile":{"type":"RuntimeDefault"}} {"allowPrivilegeEscalation":false,"readOnlyRootFilesystem":true,"runAsGroup":2001,"runAsUser":1001}
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers.
  - uid: k8s-minimize-the-admission-of-privileged-containers-namespace
    title: Minimize the admission of privileged containers
    impact: 90
    filters: |
      asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes--5.2.2
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces1.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
          k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      }
    docs:
      desc: |-
        Do not generally permit containers to be run with the `securityContext.privileged` flag set to `true`.

        **Rationale:**

        Privileged containers have access to all Linux Kernel capabilities and devices. A container running with full privileges can do almost everything that the host can do. This flag exists to allow special use-cases, like manipulating the network stack and accessing devices.

        There should be at least one admission control policy defined which does not permit privileged containers.

        If you need to run privileged containers, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `spec.containers[].securityContext.privileged: true`, `spec.initContainers[].securityContext.privileged: true` and `spec.ephemeralContainers[].securityContext.privileged: true` will not be permitted.
      audit: |-
        Run the following command:

        ```
        get pods -A -o=jsonpath=$'{range .items[*]}{@.metadata.name}: {@..securityContext}\n{end}'
        ```

        It will produce an inventory of all the privileged use on the cluster, if any (please, refer to a sample below). Further grepping can be done to automate each specific violation detection.

        calico-kube-controllers-57b57c56f-jtmk4: {} << No Elevated Privileges calico-node-c4xv4: {} {"privileged":true} {"privileged":true} {"privileged":true} {"privileged":true} << Violates 5.2.2 dashboard-metrics-scraper-7bc864c59-2m2xw: {"seccompProfile":{"type":"RuntimeDefault"}} {"allowPrivilegeEscalation":false,"readOnlyRootFilesystem":true,"runAsGroup":2001,"runAsUser":1001}
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers.
  - uid: k8s-minimize-the-admission-of-windows-hostprocess-containers-cluster
    title: Minimize the admission of Windows HostProcess Containers
    impact: 70
    filters: |
      asset.platform == "k8s-cluster"
    props:
      - uid: cis-kubernetes--5.2.11
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespaces.where(name.contains(props.excludedNamespaces10) != true)
      .all(
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
        manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      )
    docs:
      desc: |-
        Do not generally permit Windows containers to be run with the `hostProcess` flag set to true.

        **Rationale:**

        A Windows container making use of the `hostProcess` flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides "privileged access" to the Windows node.

        Where Windows containers are used inside a Kubernetes cluster, there should be at least one admission control policy which does not permit `hostProcess` Windows containers.

        If you need to run Windows containers which require `hostProcess`, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `securityContext.windowsOptions.hostProcess: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostProcess` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostProcess` containers.
  - uid: k8s-minimize-the-admission-of-windows-hostprocess-containers-namespace
    title: Minimize the admission of Windows HostProcess Containers
    impact: 70
    filters: |
      asset.family.contains("k8s-namespace")
    props:
      - uid: cis-kubernetes--5.2.11
        title: Defines a list of namespaces that should not be checked for enforcing Pod Security Standards
        mql: |
          return [
          "tigera-operator",
          "calico-system"
          ]
    mql: |
      k8s.namespace.name.lines.where(props.excludedNamespaces10.containsNone([_])) {
        k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "restricted" ||
          k8s.namespace.manifest.metadata.labels['pod-security.kubernetes.io/enforce'] == "baseline"
      }
    docs:
      desc: |-
        Do not generally permit Windows containers to be run with the `hostProcess` flag set to true.

        **Rationale:**

        A Windows container making use of the `hostProcess` flag can interact with the underlying Windows cluster node. As per the Kubernetes documentation, this provides "privileged access" to the Windows node.

        Where Windows containers are used inside a Kubernetes cluster, there should be at least one admission control policy which does not permit `hostProcess` Windows containers.

        If you need to run Windows containers which require `hostProcess`, this should be defined in a separate policy and you should carefully check to ensure that only limited service accounts and users are given permission to use that policy.

        **Impact:**

        Pods defined with `securityContext.windowsOptions.hostProcess: true` will not be permitted unless they are run under a specific policy.
      audit: List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostProcess` containers
      remediation:
        - desc: Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostProcess` containers.
  - uid: kubernetes-api-server-anonymous-auth-argument-false-apiserver
    title: Ensure that the --anonymous-auth argument is set to false
    impact: 100
    filters: processes.where( executable == /kube-apiserver/ ).list != empty
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags['anonymous-auth'] == empty
      )
    docs:
      desc: |-
        Disable anonymous requests to the Kubelet server.

        **Rationale:**

        When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests.

        **Impact:**

        Anonymous requests will be rejected.
      audit: |-
        In OpenShift 4, the kublet config file is managed by the Machine Config Operator and `anonymous-auth` is set to `false` by default.

        Run the following command on each node:

        ```

        for node in $(oc get nodes -o jsonpath='{.items[*].metadata.name}')
        do
         oc debug node/${node} -- chroot /host grep -B4 -A1 anonymous: /etc/kubernetes/kubelet.conf
        done
        ```

        Verify that the `anonymous-auth` argument is set to `false`.
      remediation: Follow the instructions in the documentation to create a Kubelet config CRD and set the `anonymous-auth` is set to `false`.
  - uid: kubernetes-api-server-anonymous-auth-argument-false-kubelet
    title: Ensure that the --anonymous-auth argument is set to false
    impact: 100
    filters: processes.where( executable == /kubelet/ ).list != empty
    mql: |
      kubelet.configuration.authentication.anonymous.enabled == false
    docs:
      desc: |-
        Disable anonymous requests to the Kubelet server.

        **Rationale:**

        When enabled, requests that are not rejected by other configured authentication methods are treated as anonymous requests. These requests are then served by the Kubelet server. You should rely on authentication to authorize access and disallow anonymous requests.

        **Impact:**

        Anonymous requests will be rejected.
      audit: |-
        In OpenShift 4, the kublet config file is managed by the Machine Config Operator and `anonymous-auth` is set to `false` by default.

        Run the following command on each node:

        ```

        for node in $(oc get nodes -o jsonpath='{.items[*].metadata.name}')
        do
         oc debug node/${node} -- chroot /host grep -B4 -A1 anonymous: /etc/kubernetes/kubelet.conf
        done
        ```

        Verify that the `anonymous-auth` argument is set to `false`.
      remediation: Follow the instructions in the documentation to create a Kubelet config CRD and set the `anonymous-auth` is set to `false`.
  - uid: kubernetes-api-server-authorization-mode-argument-not-alwaysallow-apiserver
    title: Ensure that the --authorization-mode argument is not set to AlwaysAllow
    impact: 100
    filters: processes.where(executable == /kube-apiserver/).list != empty
    mql: |
      processes.where(executable == /kube-apiserver/).all(flags["authorization-mode"] != empty)
      processes.where(executable == /kube-apiserver/).all(flags["authorization-mode"] != /AlwaysAllow/)
    docs:
      desc: |-
        Do not allow all requests. Enable explicit authorization.

        **Rationale:**

        Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests.

        **Impact:**

        Unauthorized requests will be denied.
      audit: |-
        In OpenShift 4, the kublet config file is managed by the Machine Config Operator. By default, Unauthenticated/Unauthorized users have no access to OpenShift nodes. Run the following command:

        ```

        #In one terminal, run:
         oc proxy

        #Then in another terminal, run:
        for name in $(oc get nodes -ojsonpath='{.items[*].metadata.name}')
        do
         curl -sS http://127.0.0.1:8080/api/v1/nodes/$name/proxy/configz | jq -r '.kubeletconfig.authorization.mode'
         done

        # Alternative without oc proxy
        POD=$(oc -n openshift-kube-apiserver get pod -l app=openshift-kube-apiserver -o jsonpath='{.items[0].metadata.name}')

        TOKEN=$(oc whoami -t)

        for name in $(oc get nodes -ojsonpath='{.items[*].metadata.name}')
        do
         oc exec -n openshift-kube-apiserver $POD -- curl -sS https://172.25.0.1/api/v1/nodes/$name/proxy/configz -k -H "Authorization: Bearer $TOKEN" | jq -r '.kubeletconfig.authorization.mode'
        done
        ```

        Verify that access is not successful.
      remediation: None required. Unauthenticated/Unauthorized users have no access to OpenShift nodes.
  - uid: kubernetes-api-server-authorization-mode-argument-not-alwaysallow-kubelet
    title: Ensure that the --authorization-mode argument is not set to AlwaysAllow
    impact: 100
    filters: processes.where(executable == /kubelet/).list != empty
    mql: |
      kubelet.configuration.authorization.mode != empty
      kubelet.configuration.authorization.mode != /AlwaysAllow/
    docs:
      desc: |-
        Do not allow all requests. Enable explicit authorization.

        **Rationale:**

        Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests.

        **Impact:**

        Unauthorized requests will be denied.
      audit: |-
        In OpenShift 4, the kublet config file is managed by the Machine Config Operator. By default, Unauthenticated/Unauthorized users have no access to OpenShift nodes. Run the following command:

        ```

        #In one terminal, run:
         oc proxy

        #Then in another terminal, run:
        for name in $(oc get nodes -ojsonpath='{.items[*].metadata.name}')
        do
         curl -sS http://127.0.0.1:8080/api/v1/nodes/$name/proxy/configz | jq -r '.kubeletconfig.authorization.mode'
         done

        # Alternative without oc proxy
        POD=$(oc -n openshift-kube-apiserver get pod -l app=openshift-kube-apiserver -o jsonpath='{.items[0].metadata.name}')

        TOKEN=$(oc whoami -t)

        for name in $(oc get nodes -ojsonpath='{.items[*].metadata.name}')
        do
         oc exec -n openshift-kube-apiserver $POD -- curl -sS https://172.25.0.1/api/v1/nodes/$name/proxy/configz -k -H "Authorization: Bearer $TOKEN" | jq -r '.kubeletconfig.authorization.mode'
        done
        ```

        Verify that access is not successful.
      remediation: None required. Unauthenticated/Unauthorized users have no access to OpenShift nodes.
  - uid: kubernetes-api-server-authorization-mode-argument-not-alwaysallow-kubelet-aks
    title: Ensure that the --authorization-mode argument is not set to AlwaysAllow
    impact: 100
    filters: file("/sys/class/dmi/id/chassis_asset_tag").content.trim == "7783-7084-3265-9085-8269-3286-77" && asset.platform != "amazonlinux" && asset.platform != "cos" && processes.where(executable == /kubelet/).list != empty && asset.kind != "container-image"
    mql: |
      kubelet.configuration.authentication.anonymous.enabled == "false"
      kubelet.configuration.authentication.webhook.enabled == true
    docs:
      desc: |-
        Do not allow all requests. Enable explicit authorization.

        **Rationale:**

        Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests.

        **Impact:**

        Unauthorized requests will be denied.
      audit: |-
        In OpenShift 4, the kublet config file is managed by the Machine Config Operator. By default, Unauthenticated/Unauthorized users have no access to OpenShift nodes. Run the following command:

        ```

        #In one terminal, run:
         oc proxy

        #Then in another terminal, run:
        for name in $(oc get nodes -ojsonpath='{.items[*].metadata.name}')
        do
         curl -sS http://127.0.0.1:8080/api/v1/nodes/$name/proxy/configz | jq -r '.kubeletconfig.authorization.mode'
         done

        # Alternative without oc proxy
        POD=$(oc -n openshift-kube-apiserver get pod -l app=openshift-kube-apiserver -o jsonpath='{.items[0].metadata.name}')

        TOKEN=$(oc whoami -t)

        for name in $(oc get nodes -ojsonpath='{.items[*].metadata.name}')
        do
         oc exec -n openshift-kube-apiserver $POD -- curl -sS https://172.25.0.1/api/v1/nodes/$name/proxy/configz -k -H "Authorization: Bearer $TOKEN" | jq -r '.kubeletconfig.authorization.mode'
        done
        ```

        Verify that access is not successful.
      remediation: None required. Unauthenticated/Unauthorized users have no access to OpenShift nodes.
  - uid: kubernetes-api-server-authorization-mode-argument-not-alwaysallow-kubelet-eks
    title: Ensure that the --authorization-mode argument is not set to AlwaysAllow
    impact: 100
    filters: asset.platform == "amazonlinux" && asset.version == /^2/ && processes.where(executable == /kubelet/).list != empty
    mql: |
      kubelet.configuration.authorization.mode != empty
      kubelet.configuration.authorization.mode == "Webhook"
      kubelet.configuration.authentication.webhook.enabled == true
    docs:
      desc: |-
        Do not allow all requests. Enable explicit authorization.

        **Rationale:**

        Kubelets, by default, allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests.

        **Impact:**

        Unauthorized requests will be denied.
      audit: |-
        In OpenShift 4, the kublet config file is managed by the Machine Config Operator. By default, Unauthenticated/Unauthorized users have no access to OpenShift nodes. Run the following command:

        ```

        #In one terminal, run:
         oc proxy

        #Then in another terminal, run:
        for name in $(oc get nodes -ojsonpath='{.items[*].metadata.name}')
        do
         curl -sS http://127.0.0.1:8080/api/v1/nodes/$name/proxy/configz | jq -r '.kubeletconfig.authorization.mode'
         done

        # Alternative without oc proxy
        POD=$(oc -n openshift-kube-apiserver get pod -l app=openshift-kube-apiserver -o jsonpath='{.items[0].metadata.name}')

        TOKEN=$(oc whoami -t)

        for name in $(oc get nodes -ojsonpath='{.items[*].metadata.name}')
        do
         oc exec -n openshift-kube-apiserver $POD -- curl -sS https://172.25.0.1/api/v1/nodes/$name/proxy/configz -k -H "Authorization: Bearer $TOKEN" | jq -r '.kubeletconfig.authorization.mode'
        done
        ```

        Verify that access is not successful.
      remediation: None required. Unauthenticated/Unauthorized users have no access to OpenShift nodes.
  - uid: kubernetes-ensure-tls-cert-file-tls-private-key-file-arguments-are-appropriate-apiserver
    title: Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate
    impact: 90
    filters: processes.where( executable == /kube-apiserver/ ).list != empty
    mql: |
      processes.where(executable == /kube-apiserver/).all(flags["tlsCertFile"]) != empty
      processes.where(executable == /kube-apiserver/).all(flags["tlsPrivateKeyFile"]) != empty
    docs:
      desc: |-
        Setup TLS connection on the Kubelets.

        **Rationale:**

        The connections from the `apiserver` to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the `apiserver` does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.

        **Impact:**

        TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.
      audit: |-
        By default, OpenShift uses X.509 certificates to provide secure connections between the API server and `node/kubelet`. OpenShift Container Platform monitors certificates for proper validity, for the cluster certificates it issues and manages. The OpenShift Container Platform manages certificate rotation and the alerting framework has rules to help identify when a certificate issue is about to occur.

        Run the following command on each node:

        ```
        oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.kubeletClientInfo'
        ```

        Verify that the `kubelet-client-certificate` argument is set to `/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.crt`

        Verify that the `kubelet-client-key` argument is set to `/etc/kubernetes/static-pod-certs/secrets/kublet-client/tls.key`
      remediation: OpenShift automatically manages TLS authentication for the API server communication with the `node/kublet`. This is not configurable.
  - uid: kubernetes-ensure-tls-cert-file-tls-private-key-file-arguments-are-appropriate-kubelet
    title: Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate
    impact: 90
    filters: processes.where( executable == /kubelet/ ).list != empty
    mql: |
      kubelet.configuration.tlsCertFile != empty
      kubelet.configuration.tlsPrivateKeyFile != empty
    docs:
      desc: |-
        Setup TLS connection on the Kubelets.

        **Rationale:**

        The connections from the `apiserver` to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the `apiserver` does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks.

        **Impact:**

        TLS and client certificate authentication must be configured for your Kubernetes cluster deployment.
      audit: |-
        By default, OpenShift uses X.509 certificates to provide secure connections between the API server and `node/kubelet`. OpenShift Container Platform monitors certificates for proper validity, for the cluster certificates it issues and manages. The OpenShift Container Platform manages certificate rotation and the alerting framework has rules to help identify when a certificate issue is about to occur.

        Run the following command on each node:

        ```
        oc get configmap config -n openshift-kube-apiserver -ojson | jq -r '.data["config.yaml"]' | jq '.kubeletClientInfo'
        ```

        Verify that the `kubelet-client-certificate` argument is set to `/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.crt`

        Verify that the `kubelet-client-key` argument is set to `/etc/kubernetes/static-pod-certs/secrets/kublet-client/tls.key`
      remediation: OpenShift automatically manages TLS authentication for the API server communication with the `node/kublet`. This is not configurable.
  - uid: kubernetes-kubelet-client-ca-file-argument-appropriate-apiserver
    title: Ensure that the --client-ca-file argument is set as appropriate
    impact: 90
    filters: processes.where(executable == /kube-apiserver/).list != empty
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["client-ca-file"] != empty
      )
    docs:
      desc: |-
        Enable Kubelet authentication using certificates.

        **Rationale:**

        The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests.

        **Impact:**

        You require TLS to be configured on apiserver as well as kubelets.
      audit: |-
        OpenShift provides integrated management of certificates for internal cluster components. OpenShift 4 includes multiple CAs providing independent chains of trust, which ensure that a platform CA will never accidentally sign a certificate that can be used for the wrong purpose, increasing the security posture of the cluster. The Client CA location for the kubelet is defined in `/etc/kubernetes/kubelet.conf`.

        Run the following command:

        ```

        for node in $(oc get nodes -o jsonpath='{.items[*].metadata.name}')
        do
         oc debug node/${node} -- chroot /host grep -B3 client-ca-file: /etc/systemd/system/kubelet.service
        done
        ```

        Verify that the `clientCAFile` exists and is set to `/etc/kubernetes/kubelet-ca.crt`. The output should look like the following:

        ```
        apiVersion: kubelet.config.k8s.io/v1beta1
        authentication:
         x509:
         clientCAFile: /etc/kubernetes/kubelet-ca.crt
        ```
      remediation: None required. Changing the `clientCAFile` value is unsupported.
  - uid: kubernetes-kubelet-client-ca-file-argument-appropriate-kubelet
    title: Ensure that the --client-ca-file argument is set as appropriate
    impact: 90
    filters: processes.where(executable == /kubelet/).list != empty
    mql: |
      kubelet.configuration.authentication.x509.clientCAFile != empty
    docs:
      desc: |-
        Enable Kubelet authentication using certificates.

        **Rationale:**

        The connections from the apiserver to the kubelet are used for fetching logs for pods, attaching (through kubectl) to running pods, and using the kubelet’s port-forwarding functionality. These connections terminate at the kubelet’s HTTPS endpoint. By default, the apiserver does not verify the kubelet’s serving certificate, which makes the connection subject to man-in-the-middle attacks, and unsafe to run over untrusted and/or public networks. Enabling Kubelet certificate authentication ensures that the apiserver could authenticate the Kubelet before submitting any requests.

        **Impact:**

        You require TLS to be configured on apiserver as well as kubelets.
      audit: |-
        OpenShift provides integrated management of certificates for internal cluster components. OpenShift 4 includes multiple CAs providing independent chains of trust, which ensure that a platform CA will never accidentally sign a certificate that can be used for the wrong purpose, increasing the security posture of the cluster. The Client CA location for the kubelet is defined in `/etc/kubernetes/kubelet.conf`.

        Run the following command:

        ```

        for node in $(oc get nodes -o jsonpath='{.items[*].metadata.name}')
        do
         oc debug node/${node} -- chroot /host grep -B3 client-ca-file: /etc/systemd/system/kubelet.service
        done
        ```

        Verify that the `clientCAFile` exists and is set to `/etc/kubernetes/kubelet-ca.crt`. The output should look like the following:

        ```
        apiVersion: kubelet.config.k8s.io/v1beta1
        authentication:
         x509:
         clientCAFile: /etc/kubernetes/kubelet-ca.crt
        ```
      remediation: None required. Changing the `clientCAFile` value is unsupported.
  - uid: kubernetes-profiling-argument-false-apiserver
    title: Ensure that the --profiling argument is set to false
    impact: 30
    filters: processes.where( executable == /kube-apiserver/ ).list != empty
    mql: |
      processes.where(executable == /kube-apiserver/).all(
        flags["profiling"] == "false"
      )
    docs:
      desc: |-
        Disable profiling, if not needed.

        **Rationale:**

        Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.

        **Impact:**

        Profiling information would not be available.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-scheduler
        ```

        Verify that the `--profiling` argument is set to `false`.
      remediation:
        - desc: |-
            Edit the Scheduler pod specification file `/etc/kubernetes/manifests/kube-scheduler.yaml` file on the Control Plane node and set the below parameter.

            ```
            --profiling=false
            ```
  - uid: kubernetes-profiling-argument-false-controller-manager
    title: Ensure that the --profiling argument is set to false
    impact: 30
    filters: processes.where( executable == /kube-controller-manager/ ).list != empty
    mql: |
      processes.where(executable == /kube-controller-manager/).all(
        flags["profiling"] == "false"
      )
    docs:
      desc: |-
        Disable profiling, if not needed.

        **Rationale:**

        Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.

        **Impact:**

        Profiling information would not be available.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--profiling` argument is set to `false`.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the below parameter.

            ```
            --profiling=false
            ```
  - uid: kubernetes-profiling-argument-false-kube-scheduler
    title: Ensure that the --profiling argument is set to false
    impact: 30
    filters: processes.where( executable == /kube-scheduler/ ).list != empty
    mql: |
      processes.where(executable == /kube-scheduler/).all(
        flags["profiling"] == "false"
      )
    docs:
      desc: |-
        Disable profiling, if not needed.

        **Rationale:**

        Profiling allows for the identification of specific performance bottlenecks. It generates a significant amount of program data that could potentially be exploited to uncover system and program details. If you are not experiencing any bottlenecks and do not need the profiler for troubleshooting purposes, it is recommended to turn it off to reduce the potential attack surface.

        **Impact:**

        Profiling information would not be available.
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-apiserver
        ```

        Verify that the `--profiling` argument is set to `false`.
      remediation:
        - desc: |-
            Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the below parameter.

            ```
            --profiling=false
            ```
  - uid: kubernetes-scheduler-bind-address-argument-127-0-0-1-controller-manager
    title: Ensure that the --bind-address argument is set to 127.0.0.1
    impact: 65
    filters: processes.where( executable == /kube-controller-manager/ ).list != empty
    mql: |
      processes.where(executable == /kube-controller-manager/).all(
        flags["bind-address"] == "127.0.0.1"
      )
    docs:
      desc: |-
        Do not bind the Controller Manager service to non-loopback insecure addresses.

        **Rationale:**

        The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-controller-manager
        ```

        Verify that the `--bind-address` argument is set to 127.0.0.1
      remediation:
        - desc: Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and ensure the correct value for the `--bind-address` parameter
  - uid: kubernetes-scheduler-bind-address-argument-127-0-0-1-kube-scheduler
    title: Ensure that the --bind-address argument is set to 127.0.0.1
    impact: 65
    filters: processes.where( executable == /kube-scheduler/ ).list != empty
    mql: |
      processes.where(executable == /kube-scheduler/).all(
        flags["bind-address"] == "127.0.0.1"
      )
    docs:
      desc: |-
        Do not bind the Controller Manager service to non-loopback insecure addresses.

        **Rationale:**

        The Controller Manager API service which runs on port 10252/TCP by default is used for health and metrics information and is available without authentication or encryption. As such it should only be bound to a localhost interface, to minimize the cluster's attack surface

        **Impact:**

        None
      audit: |-
        Run the following command on the Control Plane node:

        ```
        ps -ef | grep kube-controller-manager
        ```

        Verify that the `--bind-address` argument is set to 127.0.0.1
      remediation:
        - desc: Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and ensure the correct value for the `--bind-address` parameter
